{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六题(选做)：设计一种改良的优化算法\n",
    "实验内容：\n",
    "请你设计一个改进算法，能通过动态调整学习率显著提升收敛速度。\n",
    "1. 数据集不限\n",
    "2. 激活函数不限\n",
    "3. 损失函数不限\n",
    "    \n",
    "要求给出以下内容的总结：\n",
    "1. 数据集描述\n",
    "2. 预处理方法及步骤\n",
    "3. 模型架构：层数，激活函数，损失函数\n",
    "4. 神经网络超参数：学习率，迭代轮数\n",
    "5. 训练集和测试集精度\n",
    "6. 损失值变化曲线\n",
    "7. 代码注释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据描述\n",
    "\n",
    "MNIST是最有名的手写数字数据集之一，主页：http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "MNIST手写数字数据集有60000个样本组成的训练集，10000个样本组成的测试集，是NIST的子集。数字的尺寸都是归一化后的，且都在图像的中央。可以从上方的主页下载。\n",
    "\n",
    "我们使用的数据集是kaggle手写数字识别比赛中的训练集。数据集一共42000行，785列，其中第1列是标记，第2列到第785列是图像从左上角到右下角的像素值。图像大小为28×28像素，单通道的灰度图像。\n",
    "\n",
    "我们使用的是kaggle提供的MNIST手写数字识别比赛的训练集。这个数据集还是手写数字的图片，只不过像素变成了 $28 \\times 28$，图片的尺寸变大了，而且数据集的样本量也大了。我们取30%为测试集，70%为训练集。训练集样本数有29400个，测试集12600个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "本次实验使用sklearn.neural_network.MLPClassifier完成手写数字分类任务\n",
    "## 对数据进行标准化\n",
    "神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化\n",
    "\n",
    "$$X' = \\frac{X - \\bar{X}}{\\mathrm{std}(X)}$$\n",
    "\n",
    "其中，$\\bar{X}$是均值，$\\mathrm{std}$是标准差。减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  \n",
    "首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据集\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/kaggle_mnist/mnist_train.csv')\n",
    "X = data.values[:, 1:].astype('float32')\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3, random_state = 32)\n",
    "\n",
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29400, 784), (29400,), (29400, 10), (12600, 784), (12600,), (12600, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, trainY_mat.shape, testX.shape, testY.shape, testY_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对前10张图进行可视化\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, figs = plt.subplots(1, 10, figsize=(8, 4))\n",
    "for f, img, lbl in zip(figs, trainX[:10], trainY[:10]):\n",
    "    f.imshow(img.reshape((28, 28)), cmap = 'gray')\n",
    "    f.set_title(lbl)\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个标准化器的实例\n",
    "standard = StandardScaler()\n",
    "\n",
    "# 对训练集进行标准化，它会计算训练集的均值和标准差保存起来\n",
    "trainX = standard.fit_transform(trainX)\n",
    "\n",
    "# 使用标准化器在训练集上的均值和标准差，对测试集进行归一化\n",
    "testX = standard.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己实现学习率自变化\n",
    "\n",
    "沿用第四题框架仅修改train方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(h, K):\n",
    "    '''\n",
    "    参数初始化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h: int: 隐藏层单元个数\n",
    "    \n",
    "    K: int: 输出层单元个数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\"\n",
    "    \n",
    "    '''\n",
    "    np.random.seed(32)\n",
    "    W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01\n",
    "    b_1 = np.zeros((1, h))\n",
    "    \n",
    "    np.random.seed(32)\n",
    "    W_2 = np.random.normal(size = (h, K)) * 0.01\n",
    "    b_2 = np.zeros((1, K))\n",
    "    \n",
    "    parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combination(X, W, b):\n",
    "    '''\n",
    "    计算Z，Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, h)，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, h)，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, h)，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Z = XW + b\n",
    "    # YOUR CODE HERE\n",
    "    Z = X.dot(W)+b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    '''\n",
    "    ReLU激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    T = X.copy()\n",
    "    T[T<=0]=0\n",
    "    activations = T\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(O):\n",
    "    '''\n",
    "    softmax激活\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return np.exp(O)/np.sum(np.exp(O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(O):\n",
    "    '''\n",
    "    softmax激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    O: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HEER\n",
    "    activations_list = []\n",
    "    maxO_list = []\n",
    "    for i in range(0,O.shape[0]):\n",
    "        maxO_list.append(np.max(O[i]))    \n",
    "        activations_list.append(my_softmax(O[i]-maxO_list[i]))\n",
    "        \n",
    "    activations = np.array(activations_list)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(O):\n",
    "    '''\n",
    "    log softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    log_activations: np.ndarray, 激活后取了对数的矩阵\n",
    "    \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    activations_list = []\n",
    "    maxO_list = []\n",
    "    for i in range(0,O.shape[0]):\n",
    "        maxO_list.append(np.max(O[i]))\n",
    "        activations_list.append(O[i]-maxO_list[i])\n",
    "    activations1 = np.array(activations_list);\n",
    "    activations2 = np.exp(np.array(activations_list));\n",
    "    activations2 = np.sum(activations2,axis=1)\n",
    "    activations2 = activations2.reshape(len(activations2),1)\n",
    "    return activations1 - np.log(activations2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_softmax(y_true, O):\n",
    "    '''\n",
    "    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float, 平均的交叉熵损失值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 平均交叉熵损失\n",
    "    # YOUR CODE HERE\n",
    "    y_pred = log_softmax(O)\n",
    "    loss = -np.sum(y_true*y_pred) / len(y_true)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, parameters):\n",
    "    '''\n",
    "    前向传播，从输入一直到输出层softmax激活前的值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    '''\n",
    "    # 输入层到隐藏层\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    Z = X.dot(parameters['W1'])+parameters['b1']\n",
    "    \n",
    "    # 隐藏层的激活\n",
    "    # YOUR CODE HERE\n",
    "    H = ReLU(Z)\n",
    "    \n",
    "    # 隐藏层到输出层\n",
    "    # YOUR CODE HERE\n",
    "    O = H.dot(parameters['W2'])+parameters['b2']\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y_true, y_pred, H, Z, X, parameters):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    '''  \n",
    "    \n",
    "    n = len(y_true)\n",
    "    # 计算W2的梯度\n",
    "    # YOUR CODE HERE\n",
    "#     dW2 = 1.0 / n * (H.T.dot((y_pred - y_true)))\n",
    "    dW2 = np.dot(H.T, (y_pred - y_true)) / len(y_pred)\n",
    "    \n",
    "    # 计算b2的梯度\n",
    "    # YOUR CODE HERE\n",
    "#     db2 = 1.0/n * np.sum(y_pred - y_true,axis = 0)\n",
    "    db2 = np.sum(y_pred - y_true, axis = 0) / len(y_pred)\n",
    "    db2 = db2.reshape(1,len(db2))\n",
    "    \n",
    "    # 计算ReLU的梯度\n",
    "    relu_grad = Z.copy()\n",
    "    relu_grad[relu_grad < 0] = 0\n",
    "    relu_grad[relu_grad >= 0] = 1\n",
    "    \n",
    "    # 计算W1的梯度\n",
    "    # YOUR CODE HERE    \n",
    "    dW1 = X.T.dot((y_pred - y_true).dot(parameters['W2'].T)*relu_grad) / len(y_pred)\n",
    "    \n",
    "    # 计算b1的梯度\n",
    "    # YOUR CODE HERE\n",
    "    db1 = np.sum((y_pred - y_true).dot(parameters['W2'].T)*relu_grad,axis=0) / len(y_pred)\n",
    "    db1 = db1.reshape(1,len(db1))\n",
    "    grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict，参数\n",
    "    \n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    parameters['W2'] -= learning_rate * grads['dW2']\n",
    "    parameters['b2'] -= learning_rate * grads['db2']\n",
    "    parameters['W1'] -= learning_rate * grads['dW1']\n",
    "    parameters['b1'] -= learning_rate * grads['db1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y_true, y_pred, H, Z, X, parameters, learning_rate):\n",
    "    '''\n",
    "    计算梯度，参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算梯度\n",
    "    # YOUR CODE HERE\n",
    "    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)\n",
    "    \n",
    "    # 更新参数\n",
    "    # YOUR CODE HERE\n",
    "    update(parameters, grads, learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "**在此处对学习率进行修改，每当我们迭代epochs×0.1次时我们对学习率×0.1**以实现学习率的自变化使得下降速度加快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, K), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, K)，测试集的标记\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    # 存储损失值\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵\n",
    "        Z = linear_combination(trainX, parameters['W1'], parameters['b1'])\n",
    "        H = ReLU(Z)\n",
    "        train_O = linear_combination(H, parameters['W2'], parameters['b2'])\n",
    "        train_y_pred = softmax(train_O)\n",
    "        training_loss = cross_entropy_with_softmax(trainY, train_O)\n",
    "        \n",
    "        test_O = forward(testX, parameters)\n",
    "        testing_loss = cross_entropy_with_softmax(testY, test_O)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('epoch %s, training loss:%s'%(i + 1, training_loss))\n",
    "            print('epoch %s, testing loss:%s'%(i + 1, testing_loss))\n",
    "            print('epoch %s, learning_rate:%s'%(i + 1, learning_rate))\n",
    "            print()\n",
    "        \n",
    "        if (i+1)%(epochs*0.1) ==0:\n",
    "            learning_rate *= 0.1\n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)\n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制模型损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    '''\n",
    "    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prediction: np.ndarray, shape = (n, 1)，预测的标记\n",
    "    \n",
    "    '''\n",
    "    # 用forward函数得到softmax激活前的值\n",
    "    # YOUR CODE HERE\n",
    "    O = forward(X,parameters)\n",
    "    \n",
    "    # 计算softmax激活后的值\n",
    "    # YOUR CODE HERE\n",
    "    y_pred = softmax(O)\n",
    "    \n",
    "    # 取每行最大的元素对应的下标\n",
    "    # YOUR CODE HERE\n",
    "    prediction = np.argmax(y_pred,axis=1)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:2.3020861713074035\n",
      "epoch 1, testing loss:2.302147122520419\n",
      "epoch 1, learning_rate:0.8\n",
      "()\n",
      "epoch 2, training loss:2.27622033798833\n",
      "epoch 2, testing loss:2.276404801467862\n",
      "epoch 2, learning_rate:0.8\n",
      "()\n",
      "epoch 3, training loss:2.2245820217912677\n",
      "epoch 3, testing loss:2.225015414417845\n",
      "epoch 3, learning_rate:0.8\n",
      "()\n",
      "epoch 4, training loss:2.074608737415841\n",
      "epoch 4, testing loss:2.0757464764650964\n",
      "epoch 4, learning_rate:0.8\n",
      "()\n",
      "epoch 5, training loss:1.7453578843484125\n",
      "epoch 5, testing loss:1.7463874357748093\n",
      "epoch 5, learning_rate:0.8\n",
      "()\n",
      "epoch 6, training loss:1.3580337854240123\n",
      "epoch 6, testing loss:1.3608541270123455\n",
      "epoch 6, learning_rate:0.8\n",
      "()\n",
      "epoch 7, training loss:1.0700795053204704\n",
      "epoch 7, testing loss:1.0728707908157527\n",
      "epoch 7, learning_rate:0.8\n",
      "()\n",
      "epoch 8, training loss:0.9134235812282985\n",
      "epoch 8, testing loss:0.9193833557419953\n",
      "epoch 8, learning_rate:0.8\n",
      "()\n",
      "epoch 9, training loss:0.8935389548027475\n",
      "epoch 9, testing loss:0.8945414536476263\n",
      "epoch 9, learning_rate:0.8\n",
      "()\n",
      "epoch 10, training loss:0.9328616565561358\n",
      "epoch 10, testing loss:0.9481755955055499\n",
      "epoch 10, learning_rate:0.8\n",
      "()\n",
      "epoch 11, training loss:0.6373889294129264\n",
      "epoch 11, testing loss:0.6405857537633095\n",
      "epoch 11, learning_rate:0.8\n",
      "()\n",
      "epoch 12, training loss:0.5337261997659463\n",
      "epoch 12, testing loss:0.5360543805336486\n",
      "epoch 12, learning_rate:0.8\n",
      "()\n",
      "epoch 13, training loss:0.48910588467099164\n",
      "epoch 13, testing loss:0.4949548490582433\n",
      "epoch 13, learning_rate:0.8\n",
      "()\n",
      "epoch 14, training loss:0.46136239983973476\n",
      "epoch 14, testing loss:0.4669149246738341\n",
      "epoch 14, learning_rate:0.8\n",
      "()\n",
      "epoch 15, training loss:0.44590585377532205\n",
      "epoch 15, testing loss:0.45474118086596926\n",
      "epoch 15, learning_rate:0.8\n",
      "()\n",
      "epoch 16, training loss:0.4342740285082743\n",
      "epoch 16, testing loss:0.4425705392227063\n",
      "epoch 16, learning_rate:0.8\n",
      "()\n",
      "epoch 17, training loss:0.42591493281228154\n",
      "epoch 17, testing loss:0.4368366964389147\n",
      "epoch 17, learning_rate:0.8\n",
      "()\n",
      "epoch 18, training loss:0.4034816690261079\n",
      "epoch 18, testing loss:0.4144571497657897\n",
      "epoch 18, learning_rate:0.8\n",
      "()\n",
      "epoch 19, training loss:0.3862428311642817\n",
      "epoch 19, testing loss:0.3983975787565304\n",
      "epoch 19, learning_rate:0.8\n",
      "()\n",
      "epoch 20, training loss:0.3676468363137522\n",
      "epoch 20, testing loss:0.38076671366471476\n",
      "epoch 20, learning_rate:0.8\n",
      "()\n",
      "epoch 21, training loss:0.35571070541104\n",
      "epoch 21, testing loss:0.3692835846123022\n",
      "epoch 21, learning_rate:0.8\n",
      "()\n",
      "epoch 22, training loss:0.34626166033222305\n",
      "epoch 22, testing loss:0.3609070560252861\n",
      "epoch 22, learning_rate:0.8\n",
      "()\n",
      "epoch 23, training loss:0.3384156521648449\n",
      "epoch 23, testing loss:0.3535332590590369\n",
      "epoch 23, learning_rate:0.8\n",
      "()\n",
      "epoch 24, training loss:0.33205497441043014\n",
      "epoch 24, testing loss:0.3480857140018796\n",
      "epoch 24, learning_rate:0.8\n",
      "()\n",
      "epoch 25, training loss:0.3261777768622005\n",
      "epoch 25, testing loss:0.34280957370220794\n",
      "epoch 25, learning_rate:0.8\n",
      "()\n",
      "epoch 26, training loss:0.3211004684378887\n",
      "epoch 26, testing loss:0.3385158481819468\n",
      "epoch 26, learning_rate:0.8\n",
      "()\n",
      "epoch 27, training loss:0.3163282806707073\n",
      "epoch 27, testing loss:0.33439307635733523\n",
      "epoch 27, learning_rate:0.8\n",
      "()\n",
      "epoch 28, training loss:0.3120261393933566\n",
      "epoch 28, testing loss:0.33080522669723744\n",
      "epoch 28, learning_rate:0.8\n",
      "()\n",
      "epoch 29, training loss:0.3079799890393139\n",
      "epoch 29, testing loss:0.327423470432605\n",
      "epoch 29, learning_rate:0.8\n",
      "()\n",
      "epoch 30, training loss:0.3042539535331931\n",
      "epoch 30, testing loss:0.3243667865151093\n",
      "epoch 30, learning_rate:0.8\n",
      "()\n",
      "epoch 31, training loss:0.30074563261241044\n",
      "epoch 31, testing loss:0.3215019266965566\n",
      "epoch 31, learning_rate:0.8\n",
      "()\n",
      "epoch 32, training loss:0.2974694781226189\n",
      "epoch 32, testing loss:0.31886308930154983\n",
      "epoch 32, learning_rate:0.8\n",
      "()\n",
      "epoch 33, training loss:0.2943783912621953\n",
      "epoch 33, testing loss:0.3163921360633868\n",
      "epoch 33, learning_rate:0.8\n",
      "()\n",
      "epoch 34, training loss:0.2914687451941082\n",
      "epoch 34, testing loss:0.3140943911707356\n",
      "epoch 34, learning_rate:0.8\n",
      "()\n",
      "epoch 35, training loss:0.2887135573881314\n",
      "epoch 35, testing loss:0.3119401177891794\n",
      "epoch 35, learning_rate:0.8\n",
      "()\n",
      "epoch 36, training loss:0.28610293378843715\n",
      "epoch 36, testing loss:0.3099263727707937\n",
      "epoch 36, learning_rate:0.8\n",
      "()\n",
      "epoch 37, training loss:0.28362354681041174\n",
      "epoch 37, testing loss:0.30804235784335837\n",
      "epoch 37, learning_rate:0.8\n",
      "()\n",
      "epoch 38, training loss:0.2812657844905881\n",
      "epoch 38, testing loss:0.3062836533109907\n",
      "epoch 38, learning_rate:0.8\n",
      "()\n",
      "epoch 39, training loss:0.2790184364819456\n",
      "epoch 39, testing loss:0.3046280035120126\n",
      "epoch 39, learning_rate:0.8\n",
      "()\n",
      "epoch 40, training loss:0.2768751963032875\n",
      "epoch 40, testing loss:0.30306751358859\n",
      "epoch 40, learning_rate:0.8\n",
      "()\n",
      "epoch 41, training loss:0.2748263116118816\n",
      "epoch 41, testing loss:0.30159743263787464\n",
      "epoch 41, learning_rate:0.8\n",
      "()\n",
      "epoch 42, training loss:0.27286337366141505\n",
      "epoch 42, testing loss:0.3002087607681698\n",
      "epoch 42, learning_rate:0.8\n",
      "()\n",
      "epoch 43, training loss:0.2709798942319796\n",
      "epoch 43, testing loss:0.29889794725770213\n",
      "epoch 43, learning_rate:0.8\n",
      "()\n",
      "epoch 44, training loss:0.26917202362271997\n",
      "epoch 44, testing loss:0.297659070774125\n",
      "epoch 44, learning_rate:0.8\n",
      "()\n",
      "epoch 45, training loss:0.2674339146350484\n",
      "epoch 45, testing loss:0.2964831609871507\n",
      "epoch 45, learning_rate:0.8\n",
      "()\n",
      "epoch 46, training loss:0.26576220748283436\n",
      "epoch 46, testing loss:0.2953631519200999\n",
      "epoch 46, learning_rate:0.8\n",
      "()\n",
      "epoch 47, training loss:0.2641519899524162\n",
      "epoch 47, testing loss:0.2943001845686351\n",
      "epoch 47, learning_rate:0.8\n",
      "()\n",
      "epoch 48, training loss:0.2625988688594287\n",
      "epoch 48, testing loss:0.29329097504227997\n",
      "epoch 48, learning_rate:0.8\n",
      "()\n",
      "epoch 49, training loss:0.2610985395229903\n",
      "epoch 49, testing loss:0.29233098550724057\n",
      "epoch 49, learning_rate:0.8\n",
      "()\n",
      "epoch 50, training loss:0.2596467196120034\n",
      "epoch 50, testing loss:0.2914180849197755\n",
      "epoch 50, learning_rate:0.8\n",
      "()\n",
      "epoch 51, training loss:0.2595051653563243\n",
      "epoch 51, testing loss:0.29133001838598244\n",
      "epoch 51, learning_rate:0.08\n",
      "()\n",
      "epoch 52, training loss:0.2593640273969076\n",
      "epoch 52, testing loss:0.2912422690006863\n",
      "epoch 52, learning_rate:0.08\n",
      "()\n",
      "epoch 53, training loss:0.2592233378372315\n",
      "epoch 53, testing loss:0.2911549542835795\n",
      "epoch 53, learning_rate:0.08\n",
      "()\n",
      "epoch 54, training loss:0.259083098141073\n",
      "epoch 54, testing loss:0.29106807844293503\n",
      "epoch 54, learning_rate:0.08\n",
      "()\n",
      "epoch 55, training loss:0.25894332857814356\n",
      "epoch 55, testing loss:0.29098160174644805\n",
      "epoch 55, learning_rate:0.08\n",
      "()\n",
      "epoch 56, training loss:0.25880401363956596\n",
      "epoch 56, testing loss:0.29089555874766815\n",
      "epoch 56, learning_rate:0.08\n",
      "()\n",
      "epoch 57, training loss:0.25866514168047694\n",
      "epoch 57, testing loss:0.2908099441057482\n",
      "epoch 57, learning_rate:0.08\n",
      "()\n",
      "epoch 58, training loss:0.2585267581033712\n",
      "epoch 58, testing loss:0.290724791786173\n",
      "epoch 58, learning_rate:0.08\n",
      "()\n",
      "epoch 59, training loss:0.2583888133948602\n",
      "epoch 59, testing loss:0.29064006712255624\n",
      "epoch 59, learning_rate:0.08\n",
      "()\n",
      "epoch 60, training loss:0.2582512910380897\n",
      "epoch 60, testing loss:0.2905557521192437\n",
      "epoch 60, learning_rate:0.08\n",
      "()\n",
      "epoch 61, training loss:0.2581141987676324\n",
      "epoch 61, testing loss:0.29047174214316995\n",
      "epoch 61, learning_rate:0.08\n",
      "()\n",
      "epoch 62, training loss:0.257977565367211\n",
      "epoch 62, testing loss:0.29038805636749\n",
      "epoch 62, learning_rate:0.08\n",
      "()\n",
      "epoch 63, training loss:0.25784136463641394\n",
      "epoch 63, testing loss:0.29030476331095983\n",
      "epoch 63, learning_rate:0.08\n",
      "()\n",
      "epoch 64, training loss:0.2577056257750424\n",
      "epoch 64, testing loss:0.29022187922015286\n",
      "epoch 64, learning_rate:0.08\n",
      "()\n",
      "epoch 65, training loss:0.2575703280484054\n",
      "epoch 65, testing loss:0.2901393613639755\n",
      "epoch 65, learning_rate:0.08\n",
      "()\n",
      "epoch 66, training loss:0.2574354660356298\n",
      "epoch 66, testing loss:0.2900572337347475\n",
      "epoch 66, learning_rate:0.08\n",
      "()\n",
      "epoch 67, training loss:0.2573010195266548\n",
      "epoch 67, testing loss:0.28997547670218066\n",
      "epoch 67, learning_rate:0.08\n",
      "()\n",
      "epoch 68, training loss:0.2571670090156324\n",
      "epoch 68, testing loss:0.2898940707962002\n",
      "epoch 68, learning_rate:0.08\n",
      "()\n",
      "epoch 69, training loss:0.2570334112169478\n",
      "epoch 69, testing loss:0.28981296573437254\n",
      "epoch 69, learning_rate:0.08\n",
      "()\n",
      "epoch 70, training loss:0.2569002484684744\n",
      "epoch 70, testing loss:0.2897321631601788\n",
      "epoch 70, learning_rate:0.08\n",
      "()\n",
      "epoch 71, training loss:0.2567675042951316\n",
      "epoch 71, testing loss:0.28965168251752976\n",
      "epoch 71, learning_rate:0.08\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, training loss:0.2566351729967656\n",
      "epoch 72, testing loss:0.2895715904619229\n",
      "epoch 72, learning_rate:0.08\n",
      "()\n",
      "epoch 73, training loss:0.25650324220802784\n",
      "epoch 73, testing loss:0.28949190212869824\n",
      "epoch 73, learning_rate:0.08\n",
      "()\n",
      "epoch 74, training loss:0.2563717127332002\n",
      "epoch 74, testing loss:0.28941259379999124\n",
      "epoch 74, learning_rate:0.08\n",
      "()\n",
      "epoch 75, training loss:0.2562406134187025\n",
      "epoch 75, testing loss:0.28933361668454394\n",
      "epoch 75, learning_rate:0.08\n",
      "()\n",
      "epoch 76, training loss:0.25610993463465526\n",
      "epoch 76, testing loss:0.289254985529241\n",
      "epoch 76, learning_rate:0.08\n",
      "()\n",
      "epoch 77, training loss:0.25597961343974707\n",
      "epoch 77, testing loss:0.2891766668308406\n",
      "epoch 77, learning_rate:0.08\n",
      "()\n",
      "epoch 78, training loss:0.2558497107212235\n",
      "epoch 78, testing loss:0.2890986724586728\n",
      "epoch 78, learning_rate:0.08\n",
      "()\n",
      "epoch 79, training loss:0.2557202442984316\n",
      "epoch 79, testing loss:0.2890210552813424\n",
      "epoch 79, learning_rate:0.08\n",
      "()\n",
      "epoch 80, training loss:0.25559117011526433\n",
      "epoch 80, testing loss:0.28894374341172574\n",
      "epoch 80, learning_rate:0.08\n",
      "()\n",
      "epoch 81, training loss:0.25546248325718096\n",
      "epoch 81, testing loss:0.28886677319690257\n",
      "epoch 81, learning_rate:0.08\n",
      "()\n",
      "epoch 82, training loss:0.255334200025877\n",
      "epoch 82, testing loss:0.28879017301352583\n",
      "epoch 82, learning_rate:0.08\n",
      "()\n",
      "epoch 83, training loss:0.2552063267965383\n",
      "epoch 83, testing loss:0.2887138989861867\n",
      "epoch 83, learning_rate:0.08\n",
      "()\n",
      "epoch 84, training loss:0.255078818587474\n",
      "epoch 84, testing loss:0.28863796836414735\n",
      "epoch 84, learning_rate:0.08\n",
      "()\n",
      "epoch 85, training loss:0.25495168923399\n",
      "epoch 85, testing loss:0.288562396918309\n",
      "epoch 85, learning_rate:0.08\n",
      "()\n",
      "epoch 86, training loss:0.25482491348789205\n",
      "epoch 86, testing loss:0.28848713030637857\n",
      "epoch 86, learning_rate:0.08\n",
      "()\n",
      "epoch 87, training loss:0.2546985017748951\n",
      "epoch 87, testing loss:0.28841216696435606\n",
      "epoch 87, learning_rate:0.08\n",
      "()\n",
      "epoch 88, training loss:0.25457246111323173\n",
      "epoch 88, testing loss:0.2883375721709894\n",
      "epoch 88, learning_rate:0.08\n",
      "()\n",
      "epoch 89, training loss:0.2544467690097267\n",
      "epoch 89, testing loss:0.28826330463270367\n",
      "epoch 89, learning_rate:0.08\n",
      "()\n",
      "epoch 90, training loss:0.25432142680531183\n",
      "epoch 90, testing loss:0.2881893484505455\n",
      "epoch 90, learning_rate:0.08\n",
      "()\n",
      "epoch 91, training loss:0.2541964490718727\n",
      "epoch 91, testing loss:0.28811570539680015\n",
      "epoch 91, learning_rate:0.08\n",
      "()\n",
      "epoch 92, training loss:0.25407182708047027\n",
      "epoch 92, testing loss:0.288042379353273\n",
      "epoch 92, learning_rate:0.08\n",
      "()\n",
      "epoch 93, training loss:0.25394757155147896\n",
      "epoch 93, testing loss:0.28796933252839724\n",
      "epoch 93, learning_rate:0.08\n",
      "()\n",
      "epoch 94, training loss:0.2538236656906953\n",
      "epoch 94, testing loss:0.28789661133804295\n",
      "epoch 94, learning_rate:0.08\n",
      "()\n",
      "epoch 95, training loss:0.25370012901943995\n",
      "epoch 95, testing loss:0.28782416336782535\n",
      "epoch 95, learning_rate:0.08\n",
      "()\n",
      "epoch 96, training loss:0.2535769583029121\n",
      "epoch 96, testing loss:0.2877520400821414\n",
      "epoch 96, learning_rate:0.08\n",
      "()\n",
      "epoch 97, training loss:0.2534541179758094\n",
      "epoch 97, testing loss:0.28768024968466466\n",
      "epoch 97, learning_rate:0.08\n",
      "()\n",
      "epoch 98, training loss:0.2533316266520638\n",
      "epoch 98, testing loss:0.2876087750435008\n",
      "epoch 98, learning_rate:0.08\n",
      "()\n",
      "epoch 99, training loss:0.253209491455983\n",
      "epoch 99, testing loss:0.2875376287175465\n",
      "epoch 99, learning_rate:0.08\n",
      "()\n",
      "epoch 100, training loss:0.2530877110484503\n",
      "epoch 100, testing loss:0.28746678839747325\n",
      "epoch 100, learning_rate:0.08\n",
      "()\n",
      "epoch 101, training loss:0.2530755581958422\n",
      "epoch 101, testing loss:0.2874597231318092\n",
      "epoch 101, learning_rate:0.008\n",
      "()\n",
      "epoch 102, training loss:0.2530634086641776\n",
      "epoch 102, testing loss:0.2874526612170149\n",
      "epoch 102, learning_rate:0.008\n",
      "()\n",
      "epoch 103, training loss:0.25305126280817286\n",
      "epoch 103, testing loss:0.2874456020969753\n",
      "epoch 103, learning_rate:0.008\n",
      "()\n",
      "epoch 104, training loss:0.2530391200686071\n",
      "epoch 104, testing loss:0.28743854423636206\n",
      "epoch 104, learning_rate:0.008\n",
      "()\n",
      "epoch 105, training loss:0.25302698132004253\n",
      "epoch 105, testing loss:0.2874314872156553\n",
      "epoch 105, learning_rate:0.008\n",
      "()\n",
      "epoch 106, training loss:0.2530148459395734\n",
      "epoch 106, testing loss:0.2874244332373111\n",
      "epoch 106, learning_rate:0.008\n",
      "()\n",
      "epoch 107, training loss:0.2530027139037957\n",
      "epoch 107, testing loss:0.2874173824094466\n",
      "epoch 107, learning_rate:0.008\n",
      "()\n",
      "epoch 108, training loss:0.25299058549139997\n",
      "epoch 108, testing loss:0.2874103340262412\n",
      "epoch 108, learning_rate:0.008\n",
      "()\n",
      "epoch 109, training loss:0.2529784607236359\n",
      "epoch 109, testing loss:0.28740328666100157\n",
      "epoch 109, learning_rate:0.008\n",
      "()\n",
      "epoch 110, training loss:0.25296633961256726\n",
      "epoch 110, testing loss:0.28739624155927357\n",
      "epoch 110, learning_rate:0.008\n",
      "()\n",
      "epoch 111, training loss:0.2529542221921411\n",
      "epoch 111, testing loss:0.2873892002434404\n",
      "epoch 111, learning_rate:0.008\n",
      "()\n",
      "epoch 112, training loss:0.2529421083912344\n",
      "epoch 112, testing loss:0.28738216195872107\n",
      "epoch 112, learning_rate:0.008\n",
      "()\n",
      "epoch 113, training loss:0.25292999815132566\n",
      "epoch 113, testing loss:0.2873751270325651\n",
      "epoch 113, learning_rate:0.008\n",
      "()\n",
      "epoch 114, training loss:0.25291789127506\n",
      "epoch 114, testing loss:0.2873680946294391\n",
      "epoch 114, learning_rate:0.008\n",
      "()\n",
      "epoch 115, training loss:0.25290578817309783\n",
      "epoch 115, testing loss:0.28736106532559647\n",
      "epoch 115, learning_rate:0.008\n",
      "()\n",
      "epoch 116, training loss:0.2528936892357231\n",
      "epoch 116, testing loss:0.287354039159859\n",
      "epoch 116, learning_rate:0.008\n",
      "()\n",
      "epoch 117, training loss:0.25288159397484433\n",
      "epoch 117, testing loss:0.28734701585114\n",
      "epoch 117, learning_rate:0.008\n",
      "()\n",
      "epoch 118, training loss:0.2528695027403147\n",
      "epoch 118, testing loss:0.28733999473985733\n",
      "epoch 118, learning_rate:0.008\n",
      "()\n",
      "epoch 119, training loss:0.2528574150809824\n",
      "epoch 119, testing loss:0.2873329790654327\n",
      "epoch 119, learning_rate:0.008\n",
      "()\n",
      "epoch 120, training loss:0.2528453304556975\n",
      "epoch 120, testing loss:0.2873259668390364\n",
      "epoch 120, learning_rate:0.008\n",
      "()\n",
      "epoch 121, training loss:0.2528332473524859\n",
      "epoch 121, testing loss:0.2873189576480709\n",
      "epoch 121, learning_rate:0.008\n",
      "()\n",
      "epoch 122, training loss:0.25282116815456207\n",
      "epoch 122, testing loss:0.2873119516813824\n",
      "epoch 122, learning_rate:0.008\n",
      "()\n",
      "epoch 123, training loss:0.2528090923611811\n",
      "epoch 123, testing loss:0.28730494882218854\n",
      "epoch 123, learning_rate:0.008\n",
      "()\n",
      "epoch 124, training loss:0.2527970201730567\n",
      "epoch 124, testing loss:0.2872979463255175\n",
      "epoch 124, learning_rate:0.008\n",
      "()\n",
      "epoch 125, training loss:0.25278495155852543\n",
      "epoch 125, testing loss:0.28729094586409015\n",
      "epoch 125, learning_rate:0.008\n",
      "()\n",
      "epoch 126, training loss:0.2527728857184634\n",
      "epoch 126, testing loss:0.28728394824913805\n",
      "epoch 126, learning_rate:0.008\n",
      "()\n",
      "epoch 127, training loss:0.25276082269663236\n",
      "epoch 127, testing loss:0.2872769533233985\n",
      "epoch 127, learning_rate:0.008\n",
      "()\n",
      "epoch 128, training loss:0.252748762924409\n",
      "epoch 128, testing loss:0.28726996070458616\n",
      "epoch 128, learning_rate:0.008\n",
      "()\n",
      "epoch 129, training loss:0.25273670677652793\n",
      "epoch 129, testing loss:0.28726297190222955\n",
      "epoch 129, learning_rate:0.008\n",
      "()\n",
      "epoch 130, training loss:0.25272465419868567\n",
      "epoch 130, testing loss:0.2872559871105933\n",
      "epoch 130, learning_rate:0.008\n",
      "()\n",
      "epoch 131, training loss:0.25271260453457767\n",
      "epoch 131, testing loss:0.28724900345232013\n",
      "epoch 131, learning_rate:0.008\n",
      "()\n",
      "epoch 132, training loss:0.25270055596575913\n",
      "epoch 132, testing loss:0.28724202292021767\n",
      "epoch 132, learning_rate:0.008\n",
      "()\n",
      "epoch 133, training loss:0.25268851043701035\n",
      "epoch 133, testing loss:0.2872350462104789\n",
      "epoch 133, learning_rate:0.008\n",
      "()\n",
      "epoch 134, training loss:0.252676468538313\n",
      "epoch 134, testing loss:0.28722807407865547\n",
      "epoch 134, learning_rate:0.008\n",
      "()\n",
      "epoch 135, training loss:0.25266443022915036\n",
      "epoch 135, testing loss:0.2872211060287491\n",
      "epoch 135, learning_rate:0.008\n",
      "()\n",
      "epoch 136, training loss:0.2526523968774692\n",
      "epoch 136, testing loss:0.2872141409539909\n",
      "epoch 136, learning_rate:0.008\n",
      "()\n",
      "epoch 137, training loss:0.252640367203643\n",
      "epoch 137, testing loss:0.2872071789315672\n",
      "epoch 137, learning_rate:0.008\n",
      "()\n",
      "epoch 138, training loss:0.2526283409178681\n",
      "epoch 138, testing loss:0.2872002199359667\n",
      "epoch 138, learning_rate:0.008\n",
      "()\n",
      "epoch 139, training loss:0.2526163179263808\n",
      "epoch 139, testing loss:0.2871932639617858\n",
      "epoch 139, learning_rate:0.008\n",
      "()\n",
      "epoch 140, training loss:0.25260429891019326\n",
      "epoch 140, testing loss:0.28718631074318574\n",
      "epoch 140, learning_rate:0.008\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 141, training loss:0.25259228316945465\n",
      "epoch 141, testing loss:0.2871793604299746\n",
      "epoch 141, learning_rate:0.008\n",
      "()\n",
      "epoch 142, training loss:0.2525802713654298\n",
      "epoch 142, testing loss:0.2871724130634388\n",
      "epoch 142, learning_rate:0.008\n",
      "()\n",
      "epoch 143, training loss:0.2525682632664218\n",
      "epoch 143, testing loss:0.2871654681443108\n",
      "epoch 143, learning_rate:0.008\n",
      "()\n",
      "epoch 144, training loss:0.25255625876188315\n",
      "epoch 144, testing loss:0.2871585260640382\n",
      "epoch 144, learning_rate:0.008\n",
      "()\n",
      "epoch 145, training loss:0.2525442582360255\n",
      "epoch 145, testing loss:0.28715158701823956\n",
      "epoch 145, learning_rate:0.008\n",
      "()\n",
      "epoch 146, training loss:0.2525322614534414\n",
      "epoch 146, testing loss:0.2871446503806277\n",
      "epoch 146, learning_rate:0.008\n",
      "()\n",
      "epoch 147, training loss:0.25252026832886126\n",
      "epoch 147, testing loss:0.2871377168335709\n",
      "epoch 147, learning_rate:0.008\n",
      "()\n",
      "epoch 148, training loss:0.2525082789317875\n",
      "epoch 148, testing loss:0.2871307869499112\n",
      "epoch 148, learning_rate:0.008\n",
      "()\n",
      "epoch 149, training loss:0.2524962940281317\n",
      "epoch 149, testing loss:0.28712386012029983\n",
      "epoch 149, learning_rate:0.008\n",
      "()\n",
      "epoch 150, training loss:0.2524843127322707\n",
      "epoch 150, testing loss:0.28711693586745374\n",
      "epoch 150, learning_rate:0.008\n",
      "()\n",
      "epoch 151, training loss:0.252483114795462\n",
      "epoch 151, testing loss:0.28711624365022737\n",
      "epoch 151, learning_rate:0.0008\n",
      "()\n",
      "epoch 152, training loss:0.2524819166762939\n",
      "epoch 152, testing loss:0.2871155514630331\n",
      "epoch 152, learning_rate:0.0008\n",
      "()\n",
      "epoch 153, training loss:0.25248071859160814\n",
      "epoch 153, testing loss:0.2871148593058472\n",
      "epoch 153, learning_rate:0.0008\n",
      "()\n",
      "epoch 154, training loss:0.2524795205411905\n",
      "epoch 154, testing loss:0.28711416717866817\n",
      "epoch 154, learning_rate:0.0008\n",
      "()\n",
      "epoch 155, training loss:0.25247832252495317\n",
      "epoch 155, testing loss:0.2871134750814997\n",
      "epoch 155, learning_rate:0.0008\n",
      "()\n",
      "epoch 156, training loss:0.2524771245430615\n",
      "epoch 156, testing loss:0.2871127830196243\n",
      "epoch 156, learning_rate:0.0008\n",
      "()\n",
      "epoch 157, training loss:0.2524759265902975\n",
      "epoch 157, testing loss:0.28711209099003543\n",
      "epoch 157, learning_rate:0.0008\n",
      "()\n",
      "epoch 158, training loss:0.25247472866962306\n",
      "epoch 158, testing loss:0.2871113989987077\n",
      "epoch 158, learning_rate:0.0008\n",
      "()\n",
      "epoch 159, training loss:0.2524735307831894\n",
      "epoch 159, testing loss:0.2871107070532701\n",
      "epoch 159, learning_rate:0.0008\n",
      "()\n",
      "epoch 160, training loss:0.2524723329309818\n",
      "epoch 160, testing loss:0.2871100151417794\n",
      "epoch 160, learning_rate:0.0008\n",
      "()\n",
      "epoch 161, training loss:0.25247113511351693\n",
      "epoch 161, testing loss:0.28710932326133737\n",
      "epoch 161, learning_rate:0.0008\n",
      "()\n",
      "epoch 162, training loss:0.25246993733072665\n",
      "epoch 162, testing loss:0.2871086314105677\n",
      "epoch 162, learning_rate:0.0008\n",
      "()\n",
      "epoch 163, training loss:0.2524687395796662\n",
      "epoch 163, testing loss:0.28710793963013154\n",
      "epoch 163, learning_rate:0.0008\n",
      "()\n",
      "epoch 164, training loss:0.25246754186282166\n",
      "epoch 164, testing loss:0.2871072479812471\n",
      "epoch 164, learning_rate:0.0008\n",
      "()\n",
      "epoch 165, training loss:0.25246634418017133\n",
      "epoch 165, testing loss:0.28710655636234933\n",
      "epoch 165, learning_rate:0.0008\n",
      "()\n",
      "epoch 166, training loss:0.2524651465317123\n",
      "epoch 166, testing loss:0.28710586476121586\n",
      "epoch 166, learning_rate:0.0008\n",
      "()\n",
      "epoch 167, training loss:0.2524639489178155\n",
      "epoch 167, testing loss:0.2871051731757423\n",
      "epoch 167, learning_rate:0.0008\n",
      "()\n",
      "epoch 168, training loss:0.2524627513399817\n",
      "epoch 168, testing loss:0.2871044816202496\n",
      "epoch 168, learning_rate:0.0008\n",
      "()\n",
      "epoch 169, training loss:0.252461553797469\n",
      "epoch 169, testing loss:0.2871037900947366\n",
      "epoch 169, learning_rate:0.0008\n",
      "()\n",
      "epoch 170, training loss:0.2524603562891428\n",
      "epoch 170, testing loss:0.2871030985992027\n",
      "epoch 170, learning_rate:0.0008\n",
      "()\n",
      "epoch 171, training loss:0.25245915881508646\n",
      "epoch 171, testing loss:0.28710240713364615\n",
      "epoch 171, learning_rate:0.0008\n",
      "()\n",
      "epoch 172, training loss:0.25245796137524257\n",
      "epoch 172, testing loss:0.28710171570166027\n",
      "epoch 172, learning_rate:0.0008\n",
      "()\n",
      "epoch 173, training loss:0.2524567639695977\n",
      "epoch 173, testing loss:0.2871010243122988\n",
      "epoch 173, learning_rate:0.0008\n",
      "()\n",
      "epoch 174, training loss:0.25245556659777685\n",
      "epoch 174, testing loss:0.28710033295774945\n",
      "epoch 174, learning_rate:0.0008\n",
      "()\n",
      "epoch 175, training loss:0.2524543692592253\n",
      "epoch 175, testing loss:0.28709964163542623\n",
      "epoch 175, learning_rate:0.0008\n",
      "()\n",
      "epoch 176, training loss:0.25245317195313594\n",
      "epoch 176, testing loss:0.2870989503777567\n",
      "epoch 176, learning_rate:0.0008\n",
      "()\n",
      "epoch 177, training loss:0.2524519746773769\n",
      "epoch 177, testing loss:0.28709825917205944\n",
      "epoch 177, learning_rate:0.0008\n",
      "()\n",
      "epoch 178, training loss:0.25245077743186545\n",
      "epoch 178, testing loss:0.2870975679963287\n",
      "epoch 178, learning_rate:0.0008\n",
      "()\n",
      "epoch 179, training loss:0.252449580221087\n",
      "epoch 179, testing loss:0.2870968768505633\n",
      "epoch 179, learning_rate:0.0008\n",
      "()\n",
      "epoch 180, training loss:0.25244838304449085\n",
      "epoch 180, testing loss:0.2870961857347614\n",
      "epoch 180, learning_rate:0.0008\n",
      "()\n",
      "epoch 181, training loss:0.2524471858797263\n",
      "epoch 181, testing loss:0.28709549464892153\n",
      "epoch 181, learning_rate:0.0008\n",
      "()\n",
      "epoch 182, training loss:0.25244598872296353\n",
      "epoch 182, testing loss:0.2870948035930413\n",
      "epoch 182, learning_rate:0.0008\n",
      "()\n",
      "epoch 183, training loss:0.2524447916003656\n",
      "epoch 183, testing loss:0.2870941125671185\n",
      "epoch 183, learning_rate:0.0008\n",
      "()\n",
      "epoch 184, training loss:0.25244359450892606\n",
      "epoch 184, testing loss:0.2870934215745998\n",
      "epoch 184, learning_rate:0.0008\n",
      "()\n",
      "epoch 185, training loss:0.2524423974492007\n",
      "epoch 185, testing loss:0.28709273061286195\n",
      "epoch 185, learning_rate:0.0008\n",
      "()\n",
      "epoch 186, training loss:0.2524412004226538\n",
      "epoch 186, testing loss:0.287092039687131\n",
      "epoch 186, learning_rate:0.0008\n",
      "()\n",
      "epoch 187, training loss:0.2524400034304153\n",
      "epoch 187, testing loss:0.2870913488237894\n",
      "epoch 187, learning_rate:0.0008\n",
      "()\n",
      "epoch 188, training loss:0.25243880647342676\n",
      "epoch 188, testing loss:0.2870906579932049\n",
      "epoch 188, learning_rate:0.0008\n",
      "()\n",
      "epoch 189, training loss:0.25243760955054667\n",
      "epoch 189, testing loss:0.2870899671935897\n",
      "epoch 189, learning_rate:0.0008\n",
      "()\n",
      "epoch 190, training loss:0.25243641266295674\n",
      "epoch 190, testing loss:0.287089276256191\n",
      "epoch 190, learning_rate:0.0008\n",
      "()\n",
      "epoch 191, training loss:0.2524352158108851\n",
      "epoch 191, testing loss:0.28708858520636193\n",
      "epoch 191, learning_rate:0.0008\n",
      "()\n",
      "epoch 192, training loss:0.25243401899327667\n",
      "epoch 192, testing loss:0.2870878940601602\n",
      "epoch 192, learning_rate:0.0008\n",
      "()\n",
      "epoch 193, training loss:0.2524328222086763\n",
      "epoch 193, testing loss:0.2870872028154102\n",
      "epoch 193, learning_rate:0.0008\n",
      "()\n",
      "epoch 194, training loss:0.2524316254466688\n",
      "epoch 194, testing loss:0.2870865116006057\n",
      "epoch 194, learning_rate:0.0008\n",
      "()\n",
      "epoch 195, training loss:0.2524304287047814\n",
      "epoch 195, testing loss:0.28708582041777336\n",
      "epoch 195, learning_rate:0.0008\n",
      "()\n",
      "epoch 196, training loss:0.25242923199744094\n",
      "epoch 196, testing loss:0.2870851292667053\n",
      "epoch 196, learning_rate:0.0008\n",
      "()\n",
      "epoch 197, training loss:0.25242803532566904\n",
      "epoch 197, testing loss:0.2870844381460063\n",
      "epoch 197, learning_rate:0.0008\n",
      "()\n",
      "epoch 198, training loss:0.2524268386880382\n",
      "epoch 198, testing loss:0.28708374705474393\n",
      "epoch 198, learning_rate:0.0008\n",
      "()\n",
      "epoch 199, training loss:0.2524256420851054\n",
      "epoch 199, testing loss:0.2870830559934733\n",
      "epoch 199, learning_rate:0.0008\n",
      "()\n",
      "epoch 200, training loss:0.2524244455164414\n",
      "epoch 200, testing loss:0.2870823649603803\n",
      "epoch 200, learning_rate:0.0008\n",
      "()\n",
      "epoch 201, training loss:0.2524243258622019\n",
      "epoch 201, testing loss:0.2870822958575765\n",
      "epoch 201, learning_rate:8e-05\n",
      "()\n",
      "epoch 202, training loss:0.25242420620830386\n",
      "epoch 202, testing loss:0.287082226755072\n",
      "epoch 202, learning_rate:8e-05\n",
      "()\n",
      "epoch 203, training loss:0.2524240865547472\n",
      "epoch 203, testing loss:0.28708215765286693\n",
      "epoch 203, learning_rate:8e-05\n",
      "()\n",
      "epoch 204, training loss:0.25242396690153174\n",
      "epoch 204, testing loss:0.28708208855096107\n",
      "epoch 204, learning_rate:8e-05\n",
      "()\n",
      "epoch 205, training loss:0.25242384724865724\n",
      "epoch 205, testing loss:0.28708201944935463\n",
      "epoch 205, learning_rate:8e-05\n",
      "()\n",
      "epoch 206, training loss:0.25242372759612364\n",
      "epoch 206, testing loss:0.28708195034804745\n",
      "epoch 206, learning_rate:8e-05\n",
      "()\n",
      "epoch 207, training loss:0.25242360794393137\n",
      "epoch 207, testing loss:0.2870818812470397\n",
      "epoch 207, learning_rate:8e-05\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 208, training loss:0.25242348829208056\n",
      "epoch 208, testing loss:0.28708181214633116\n",
      "epoch 208, learning_rate:8e-05\n",
      "()\n",
      "epoch 209, training loss:0.252423368640571\n",
      "epoch 209, testing loss:0.2870817430459221\n",
      "epoch 209, learning_rate:8e-05\n",
      "()\n",
      "epoch 210, training loss:0.2524232489894028\n",
      "epoch 210, testing loss:0.28708167394581224\n",
      "epoch 210, learning_rate:8e-05\n",
      "()\n",
      "epoch 211, training loss:0.2524231293385759\n",
      "epoch 211, testing loss:0.28708160484600176\n",
      "epoch 211, learning_rate:8e-05\n",
      "()\n",
      "epoch 212, training loss:0.25242300968809034\n",
      "epoch 212, testing loss:0.28708153574649053\n",
      "epoch 212, learning_rate:8e-05\n",
      "()\n",
      "epoch 213, training loss:0.25242289003794616\n",
      "epoch 213, testing loss:0.2870814666472788\n",
      "epoch 213, learning_rate:8e-05\n",
      "()\n",
      "epoch 214, training loss:0.2524227703881433\n",
      "epoch 214, testing loss:0.2870813975483662\n",
      "epoch 214, learning_rate:8e-05\n",
      "()\n",
      "epoch 215, training loss:0.25242265073868186\n",
      "epoch 215, testing loss:0.28708132844975304\n",
      "epoch 215, learning_rate:8e-05\n",
      "()\n",
      "epoch 216, training loss:0.2524225310895617\n",
      "epoch 216, testing loss:0.2870812593513577\n",
      "epoch 216, learning_rate:8e-05\n",
      "()\n",
      "epoch 217, training loss:0.25242241144078276\n",
      "epoch 217, testing loss:0.287081190252663\n",
      "epoch 217, learning_rate:8e-05\n",
      "()\n",
      "epoch 218, training loss:0.2524222917923452\n",
      "epoch 218, testing loss:0.2870811211542676\n",
      "epoch 218, learning_rate:8e-05\n",
      "()\n",
      "epoch 219, training loss:0.2524221721442489\n",
      "epoch 219, testing loss:0.28708105205617146\n",
      "epoch 219, learning_rate:8e-05\n",
      "()\n",
      "epoch 220, training loss:0.2524220524964941\n",
      "epoch 220, testing loss:0.28708098295837475\n",
      "epoch 220, learning_rate:8e-05\n",
      "()\n",
      "epoch 221, training loss:0.2524219328490806\n",
      "epoch 221, testing loss:0.28708091386087725\n",
      "epoch 221, learning_rate:8e-05\n",
      "()\n",
      "epoch 222, training loss:0.2524218132020415\n",
      "epoch 222, testing loss:0.2870808447636792\n",
      "epoch 222, learning_rate:8e-05\n",
      "()\n",
      "epoch 223, training loss:0.2524216935553439\n",
      "epoch 223, testing loss:0.2870807756667803\n",
      "epoch 223, learning_rate:8e-05\n",
      "()\n",
      "epoch 224, training loss:0.2524215739089875\n",
      "epoch 224, testing loss:0.2870807065701808\n",
      "epoch 224, learning_rate:8e-05\n",
      "()\n",
      "epoch 225, training loss:0.2524214542629724\n",
      "epoch 225, testing loss:0.2870806374738806\n",
      "epoch 225, learning_rate:8e-05\n",
      "()\n",
      "epoch 226, training loss:0.2524213346172986\n",
      "epoch 226, testing loss:0.287080568376802\n",
      "epoch 226, learning_rate:8e-05\n",
      "()\n",
      "epoch 227, training loss:0.2524212149719661\n",
      "epoch 227, testing loss:0.2870804992639812\n",
      "epoch 227, learning_rate:8e-05\n",
      "()\n",
      "epoch 228, training loss:0.2524210953268797\n",
      "epoch 228, testing loss:0.28708043015146\n",
      "epoch 228, learning_rate:8e-05\n",
      "()\n",
      "epoch 229, training loss:0.2524209756820585\n",
      "epoch 229, testing loss:0.287080361039238\n",
      "epoch 229, learning_rate:8e-05\n",
      "()\n",
      "epoch 230, training loss:0.2524208560375721\n",
      "epoch 230, testing loss:0.28708029192731555\n",
      "epoch 230, learning_rate:8e-05\n",
      "()\n",
      "epoch 231, training loss:0.252420736393451\n",
      "epoch 231, testing loss:0.2870802228156923\n",
      "epoch 231, learning_rate:8e-05\n",
      "()\n",
      "epoch 232, training loss:0.2524206167496813\n",
      "epoch 232, testing loss:0.2870801537043685\n",
      "epoch 232, learning_rate:8e-05\n",
      "()\n",
      "epoch 233, training loss:0.2524204971062528\n",
      "epoch 233, testing loss:0.28708008459334416\n",
      "epoch 233, learning_rate:8e-05\n",
      "()\n",
      "epoch 234, training loss:0.2524203774632563\n",
      "epoch 234, testing loss:0.2870800154826191\n",
      "epoch 234, learning_rate:8e-05\n",
      "()\n",
      "epoch 235, training loss:0.252420257822143\n",
      "epoch 235, testing loss:0.2870799463721934\n",
      "epoch 235, learning_rate:8e-05\n",
      "()\n",
      "epoch 236, training loss:0.25242013818137093\n",
      "epoch 236, testing loss:0.2870798772620672\n",
      "epoch 236, learning_rate:8e-05\n",
      "()\n",
      "epoch 237, training loss:0.25242001854094037\n",
      "epoch 237, testing loss:0.2870798081522404\n",
      "epoch 237, learning_rate:8e-05\n",
      "()\n",
      "epoch 238, training loss:0.252419898900851\n",
      "epoch 238, testing loss:0.28707973904271283\n",
      "epoch 238, learning_rate:8e-05\n",
      "()\n",
      "epoch 239, training loss:0.2524197792611028\n",
      "epoch 239, testing loss:0.28707966993348466\n",
      "epoch 239, learning_rate:8e-05\n",
      "()\n",
      "epoch 240, training loss:0.2524196596217171\n",
      "epoch 240, testing loss:0.28707960082455586\n",
      "epoch 240, learning_rate:8e-05\n",
      "()\n",
      "epoch 241, training loss:0.25241953998284106\n",
      "epoch 241, testing loss:0.2870795317159265\n",
      "epoch 241, learning_rate:8e-05\n",
      "()\n",
      "epoch 242, training loss:0.25241942034430637\n",
      "epoch 242, testing loss:0.2870794626075964\n",
      "epoch 242, learning_rate:8e-05\n",
      "()\n",
      "epoch 243, training loss:0.25241930070611296\n",
      "epoch 243, testing loss:0.2870793934995657\n",
      "epoch 243, learning_rate:8e-05\n",
      "()\n",
      "epoch 244, training loss:0.2524191810682608\n",
      "epoch 244, testing loss:0.28707932439183437\n",
      "epoch 244, learning_rate:8e-05\n",
      "()\n",
      "epoch 245, training loss:0.2524190614307499\n",
      "epoch 245, testing loss:0.2870792552844024\n",
      "epoch 245, learning_rate:8e-05\n",
      "()\n",
      "epoch 246, training loss:0.2524189417935803\n",
      "epoch 246, testing loss:0.2870791861772698\n",
      "epoch 246, learning_rate:8e-05\n",
      "()\n",
      "epoch 247, training loss:0.252418822156752\n",
      "epoch 247, testing loss:0.2870791170704366\n",
      "epoch 247, learning_rate:8e-05\n",
      "()\n",
      "epoch 248, training loss:0.252418702520265\n",
      "epoch 248, testing loss:0.28707904796390266\n",
      "epoch 248, learning_rate:8e-05\n",
      "()\n",
      "epoch 249, training loss:0.25241858288411917\n",
      "epoch 249, testing loss:0.2870789788576695\n",
      "epoch 249, learning_rate:8e-05\n",
      "()\n",
      "epoch 250, training loss:0.2524184632483147\n",
      "epoch 250, testing loss:0.28707890975173905\n",
      "epoch 250, learning_rate:8e-05\n",
      "()\n",
      "epoch 251, training loss:0.2524184512847605\n",
      "epoch 251, testing loss:0.2870789028411672\n",
      "epoch 251, learning_rate:8e-06\n",
      "()\n",
      "epoch 252, training loss:0.2524184393212096\n",
      "epoch 252, testing loss:0.2870788959305983\n",
      "epoch 252, learning_rate:8e-06\n",
      "()\n",
      "epoch 253, training loss:0.25241842735766223\n",
      "epoch 253, testing loss:0.2870788890200324\n",
      "epoch 253, learning_rate:8e-06\n",
      "()\n",
      "epoch 254, training loss:0.2524184153941183\n",
      "epoch 254, testing loss:0.28707888210946964\n",
      "epoch 254, learning_rate:8e-06\n",
      "()\n",
      "epoch 255, training loss:0.2524184034305778\n",
      "epoch 255, testing loss:0.2870788751989097\n",
      "epoch 255, learning_rate:8e-06\n",
      "()\n",
      "epoch 256, training loss:0.25241839146704076\n",
      "epoch 256, testing loss:0.28707886828835283\n",
      "epoch 256, learning_rate:8e-06\n",
      "()\n",
      "epoch 257, training loss:0.25241837950350693\n",
      "epoch 257, testing loss:0.2870788613777989\n",
      "epoch 257, learning_rate:8e-06\n",
      "()\n",
      "epoch 258, training loss:0.25241836753997665\n",
      "epoch 258, testing loss:0.287078854467248\n",
      "epoch 258, learning_rate:8e-06\n",
      "()\n",
      "epoch 259, training loss:0.2524183555764498\n",
      "epoch 259, testing loss:0.28707884755670016\n",
      "epoch 259, learning_rate:8e-06\n",
      "()\n",
      "epoch 260, training loss:0.25241834361292637\n",
      "epoch 260, testing loss:0.28707884064615524\n",
      "epoch 260, learning_rate:8e-06\n",
      "()\n",
      "epoch 261, training loss:0.2524183316494063\n",
      "epoch 261, testing loss:0.2870788337356134\n",
      "epoch 261, learning_rate:8e-06\n",
      "()\n",
      "epoch 262, training loss:0.25241831968588957\n",
      "epoch 262, testing loss:0.287078826825075\n",
      "epoch 262, learning_rate:8e-06\n",
      "()\n",
      "epoch 263, training loss:0.2524183077223763\n",
      "epoch 263, testing loss:0.2870788199145416\n",
      "epoch 263, learning_rate:8e-06\n",
      "()\n",
      "epoch 264, training loss:0.25241829575886654\n",
      "epoch 264, testing loss:0.2870788130040112\n",
      "epoch 264, learning_rate:8e-06\n",
      "()\n",
      "epoch 265, training loss:0.25241828379536013\n",
      "epoch 265, testing loss:0.28707880609348385\n",
      "epoch 265, learning_rate:8e-06\n",
      "()\n",
      "epoch 266, training loss:0.25241827183185717\n",
      "epoch 266, testing loss:0.2870787991829594\n",
      "epoch 266, learning_rate:8e-06\n",
      "()\n",
      "epoch 267, training loss:0.2524182598683576\n",
      "epoch 267, testing loss:0.28707879227243793\n",
      "epoch 267, learning_rate:8e-06\n",
      "()\n",
      "epoch 268, training loss:0.2524182479048614\n",
      "epoch 268, testing loss:0.2870787853619195\n",
      "epoch 268, learning_rate:8e-06\n",
      "()\n",
      "epoch 269, training loss:0.2524182359413686\n",
      "epoch 269, testing loss:0.28707877845140406\n",
      "epoch 269, learning_rate:8e-06\n",
      "()\n",
      "epoch 270, training loss:0.25241822397787916\n",
      "epoch 270, testing loss:0.28707877154089156\n",
      "epoch 270, learning_rate:8e-06\n",
      "()\n",
      "epoch 271, training loss:0.2524182120143932\n",
      "epoch 271, testing loss:0.2870787646303822\n",
      "epoch 271, learning_rate:8e-06\n",
      "()\n",
      "epoch 272, training loss:0.25241820005091087\n",
      "epoch 272, testing loss:0.2870787577198757\n",
      "epoch 272, learning_rate:8e-06\n",
      "()\n",
      "epoch 273, training loss:0.2524181880874317\n",
      "epoch 273, testing loss:0.2870787508093722\n",
      "epoch 273, learning_rate:8e-06\n",
      "()\n",
      "epoch 274, training loss:0.2524181761239559\n",
      "epoch 274, testing loss:0.2870787438988717\n",
      "epoch 274, learning_rate:8e-06\n",
      "()\n",
      "epoch 275, training loss:0.2524181641604837\n",
      "epoch 275, testing loss:0.2870787369883742\n",
      "epoch 275, learning_rate:8e-06\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 276, training loss:0.2524181521970148\n",
      "epoch 276, testing loss:0.28707873007787976\n",
      "epoch 276, learning_rate:8e-06\n",
      "()\n",
      "epoch 277, training loss:0.25241814023354936\n",
      "epoch 277, testing loss:0.2870787231673883\n",
      "epoch 277, learning_rate:8e-06\n",
      "()\n",
      "epoch 278, training loss:0.2524181282700873\n",
      "epoch 278, testing loss:0.2870787162568998\n",
      "epoch 278, learning_rate:8e-06\n",
      "()\n",
      "epoch 279, training loss:0.25241811630662864\n",
      "epoch 279, testing loss:0.28707870934641433\n",
      "epoch 279, learning_rate:8e-06\n",
      "()\n",
      "epoch 280, training loss:0.2524181043431735\n",
      "epoch 280, testing loss:0.28707870243593175\n",
      "epoch 280, learning_rate:8e-06\n",
      "()\n",
      "epoch 281, training loss:0.2524180923797217\n",
      "epoch 281, testing loss:0.2870786955254522\n",
      "epoch 281, learning_rate:8e-06\n",
      "()\n",
      "epoch 282, training loss:0.2524180804162732\n",
      "epoch 282, testing loss:0.2870786886149757\n",
      "epoch 282, learning_rate:8e-06\n",
      "()\n",
      "epoch 283, training loss:0.2524180684528282\n",
      "epoch 283, testing loss:0.2870786817045021\n",
      "epoch 283, learning_rate:8e-06\n",
      "()\n",
      "epoch 284, training loss:0.25241805648938676\n",
      "epoch 284, testing loss:0.28707867479403154\n",
      "epoch 284, learning_rate:8e-06\n",
      "()\n",
      "epoch 285, training loss:0.25241804452594857\n",
      "epoch 285, testing loss:0.28707866788356406\n",
      "epoch 285, learning_rate:8e-06\n",
      "()\n",
      "epoch 286, training loss:0.25241803256251377\n",
      "epoch 286, testing loss:0.28707866097309953\n",
      "epoch 286, learning_rate:8e-06\n",
      "()\n",
      "epoch 287, training loss:0.25241802059908247\n",
      "epoch 287, testing loss:0.28707865406263794\n",
      "epoch 287, learning_rate:8e-06\n",
      "()\n",
      "epoch 288, training loss:0.2524180086356545\n",
      "epoch 288, testing loss:0.2870786471521793\n",
      "epoch 288, learning_rate:8e-06\n",
      "()\n",
      "epoch 289, training loss:0.25241799667222997\n",
      "epoch 289, testing loss:0.28707864024172375\n",
      "epoch 289, learning_rate:8e-06\n",
      "()\n",
      "epoch 290, training loss:0.25241798470880894\n",
      "epoch 290, testing loss:0.28707863333127115\n",
      "epoch 290, learning_rate:8e-06\n",
      "()\n",
      "epoch 291, training loss:0.2524179727453912\n",
      "epoch 291, testing loss:0.28707862642082166\n",
      "epoch 291, learning_rate:8e-06\n",
      "()\n",
      "epoch 292, training loss:0.252417960781977\n",
      "epoch 292, testing loss:0.28707861951037494\n",
      "epoch 292, learning_rate:8e-06\n",
      "()\n",
      "epoch 293, training loss:0.2524179488185661\n",
      "epoch 293, testing loss:0.2870786125999313\n",
      "epoch 293, learning_rate:8e-06\n",
      "()\n",
      "epoch 294, training loss:0.2524179368551587\n",
      "epoch 294, testing loss:0.28707860568949073\n",
      "epoch 294, learning_rate:8e-06\n",
      "()\n",
      "epoch 295, training loss:0.25241792489175463\n",
      "epoch 295, testing loss:0.2870785987790532\n",
      "epoch 295, learning_rate:8e-06\n",
      "()\n",
      "epoch 296, training loss:0.2524179129283541\n",
      "epoch 296, testing loss:0.28707859186861856\n",
      "epoch 296, learning_rate:8e-06\n",
      "()\n",
      "epoch 297, training loss:0.2524179009649568\n",
      "epoch 297, testing loss:0.2870785849581869\n",
      "epoch 297, learning_rate:8e-06\n",
      "()\n",
      "epoch 298, training loss:0.252417889001563\n",
      "epoch 298, testing loss:0.28707857804775827\n",
      "epoch 298, learning_rate:8e-06\n",
      "()\n",
      "epoch 299, training loss:0.25241787703817264\n",
      "epoch 299, testing loss:0.2870785711373327\n",
      "epoch 299, learning_rate:8e-06\n",
      "()\n",
      "epoch 300, training loss:0.25241786507478564\n",
      "epoch 300, testing loss:0.2870785642269099\n",
      "epoch 300, learning_rate:8e-06\n",
      "()\n",
      "epoch 301, training loss:0.2524178638784473\n",
      "epoch 301, testing loss:0.2870785635358679\n",
      "epoch 301, learning_rate:8e-07\n",
      "()\n",
      "epoch 302, training loss:0.2524178626821088\n",
      "epoch 302, testing loss:0.28707856284482586\n",
      "epoch 302, learning_rate:8e-07\n",
      "()\n",
      "epoch 303, training loss:0.2524178614857705\n",
      "epoch 303, testing loss:0.28707856215378386\n",
      "epoch 303, learning_rate:8e-07\n",
      "()\n",
      "epoch 304, training loss:0.25241786028943214\n",
      "epoch 304, testing loss:0.2870785614627419\n",
      "epoch 304, learning_rate:8e-07\n",
      "()\n",
      "epoch 305, training loss:0.2524178590930939\n",
      "epoch 305, testing loss:0.2870785607717\n",
      "epoch 305, learning_rate:8e-07\n",
      "()\n",
      "epoch 306, training loss:0.25241785789675564\n",
      "epoch 306, testing loss:0.2870785600806581\n",
      "epoch 306, learning_rate:8e-07\n",
      "()\n",
      "epoch 307, training loss:0.25241785670041733\n",
      "epoch 307, testing loss:0.28707855938961624\n",
      "epoch 307, learning_rate:8e-07\n",
      "()\n",
      "epoch 308, training loss:0.2524178555040792\n",
      "epoch 308, testing loss:0.28707855869857435\n",
      "epoch 308, learning_rate:8e-07\n",
      "()\n",
      "epoch 309, training loss:0.252417854307741\n",
      "epoch 309, testing loss:0.2870785580075326\n",
      "epoch 309, learning_rate:8e-07\n",
      "()\n",
      "epoch 310, training loss:0.2524178531114029\n",
      "epoch 310, testing loss:0.2870785573164907\n",
      "epoch 310, learning_rate:8e-07\n",
      "()\n",
      "epoch 311, training loss:0.25241785191506483\n",
      "epoch 311, testing loss:0.28707855662544907\n",
      "epoch 311, learning_rate:8e-07\n",
      "()\n",
      "epoch 312, training loss:0.25241785071872674\n",
      "epoch 312, testing loss:0.2870785559344073\n",
      "epoch 312, learning_rate:8e-07\n",
      "()\n",
      "epoch 313, training loss:0.2524178495223887\n",
      "epoch 313, testing loss:0.2870785552433656\n",
      "epoch 313, learning_rate:8e-07\n",
      "()\n",
      "epoch 314, training loss:0.2524178483260507\n",
      "epoch 314, testing loss:0.28707855455232384\n",
      "epoch 314, learning_rate:8e-07\n",
      "()\n",
      "epoch 315, training loss:0.2524178471297128\n",
      "epoch 315, testing loss:0.28707855386128234\n",
      "epoch 315, learning_rate:8e-07\n",
      "()\n",
      "epoch 316, training loss:0.2524178459333748\n",
      "epoch 316, testing loss:0.2870785531702407\n",
      "epoch 316, learning_rate:8e-07\n",
      "()\n",
      "epoch 317, training loss:0.25241784473703693\n",
      "epoch 317, testing loss:0.28707855247919906\n",
      "epoch 317, learning_rate:8e-07\n",
      "()\n",
      "epoch 318, training loss:0.2524178435406991\n",
      "epoch 318, testing loss:0.2870785517881575\n",
      "epoch 318, learning_rate:8e-07\n",
      "()\n",
      "epoch 319, training loss:0.2524178423443613\n",
      "epoch 319, testing loss:0.28707855109711605\n",
      "epoch 319, learning_rate:8e-07\n",
      "()\n",
      "epoch 320, training loss:0.2524178411480235\n",
      "epoch 320, testing loss:0.2870785504060746\n",
      "epoch 320, learning_rate:8e-07\n",
      "()\n",
      "epoch 321, training loss:0.25241783995168565\n",
      "epoch 321, testing loss:0.2870785497150331\n",
      "epoch 321, learning_rate:8e-07\n",
      "()\n",
      "epoch 322, training loss:0.252417838755348\n",
      "epoch 322, testing loss:0.28707854902399166\n",
      "epoch 322, learning_rate:8e-07\n",
      "()\n",
      "epoch 323, training loss:0.2524178375590104\n",
      "epoch 323, testing loss:0.2870785483329502\n",
      "epoch 323, learning_rate:8e-07\n",
      "()\n",
      "epoch 324, training loss:0.25241783636267273\n",
      "epoch 324, testing loss:0.28707854764190893\n",
      "epoch 324, learning_rate:8e-07\n",
      "()\n",
      "epoch 325, training loss:0.2524178351663351\n",
      "epoch 325, testing loss:0.2870785469508676\n",
      "epoch 325, learning_rate:8e-07\n",
      "()\n",
      "epoch 326, training loss:0.2524178339699975\n",
      "epoch 326, testing loss:0.28707854625982626\n",
      "epoch 326, learning_rate:8e-07\n",
      "()\n",
      "epoch 327, training loss:0.2524178327736599\n",
      "epoch 327, testing loss:0.28707854556878504\n",
      "epoch 327, learning_rate:8e-07\n",
      "()\n",
      "epoch 328, training loss:0.2524178315773224\n",
      "epoch 328, testing loss:0.2870785448777438\n",
      "epoch 328, learning_rate:8e-07\n",
      "()\n",
      "epoch 329, training loss:0.252417830380985\n",
      "epoch 329, testing loss:0.28707854418670253\n",
      "epoch 329, learning_rate:8e-07\n",
      "()\n",
      "epoch 330, training loss:0.25241782918464745\n",
      "epoch 330, testing loss:0.2870785434956614\n",
      "epoch 330, learning_rate:8e-07\n",
      "()\n",
      "epoch 331, training loss:0.2524178279883101\n",
      "epoch 331, testing loss:0.28707854280462025\n",
      "epoch 331, learning_rate:8e-07\n",
      "()\n",
      "epoch 332, training loss:0.25241782679197267\n",
      "epoch 332, testing loss:0.28707854211357914\n",
      "epoch 332, learning_rate:8e-07\n",
      "()\n",
      "epoch 333, training loss:0.2524178255956354\n",
      "epoch 333, testing loss:0.287078541422538\n",
      "epoch 333, learning_rate:8e-07\n",
      "()\n",
      "epoch 334, training loss:0.25241782439929805\n",
      "epoch 334, testing loss:0.28707854073149697\n",
      "epoch 334, learning_rate:8e-07\n",
      "()\n",
      "epoch 335, training loss:0.2524178232029608\n",
      "epoch 335, testing loss:0.2870785400404559\n",
      "epoch 335, learning_rate:8e-07\n",
      "()\n",
      "epoch 336, training loss:0.25241782200662355\n",
      "epoch 336, testing loss:0.28707853934941485\n",
      "epoch 336, learning_rate:8e-07\n",
      "()\n",
      "epoch 337, training loss:0.25241782081028635\n",
      "epoch 337, testing loss:0.2870785386583739\n",
      "epoch 337, learning_rate:8e-07\n",
      "()\n",
      "epoch 338, training loss:0.2524178196139493\n",
      "epoch 338, testing loss:0.28707853796733307\n",
      "epoch 338, learning_rate:8e-07\n",
      "()\n",
      "epoch 339, training loss:0.252417818417612\n",
      "epoch 339, testing loss:0.28707853727629207\n",
      "epoch 339, learning_rate:8e-07\n",
      "()\n",
      "epoch 340, training loss:0.252417817221275\n",
      "epoch 340, testing loss:0.2870785365852512\n",
      "epoch 340, learning_rate:8e-07\n",
      "()\n",
      "epoch 341, training loss:0.2524178160249379\n",
      "epoch 341, testing loss:0.28707853589421034\n",
      "epoch 341, learning_rate:8e-07\n",
      "()\n",
      "epoch 342, training loss:0.2524178148286009\n",
      "epoch 342, testing loss:0.2870785352031695\n",
      "epoch 342, learning_rate:8e-07\n",
      "()\n",
      "epoch 343, training loss:0.25241781363226384\n",
      "epoch 343, testing loss:0.28707853451212867\n",
      "epoch 343, learning_rate:8e-07\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 344, training loss:0.25241781243592687\n",
      "epoch 344, testing loss:0.28707853382108794\n",
      "epoch 344, learning_rate:8e-07\n",
      "()\n",
      "epoch 345, training loss:0.2524178112395899\n",
      "epoch 345, testing loss:0.28707853313004716\n",
      "epoch 345, learning_rate:8e-07\n",
      "()\n",
      "epoch 346, training loss:0.2524178100432531\n",
      "epoch 346, testing loss:0.2870785324390065\n",
      "epoch 346, learning_rate:8e-07\n",
      "()\n",
      "epoch 347, training loss:0.25241780884691617\n",
      "epoch 347, testing loss:0.28707853174796577\n",
      "epoch 347, learning_rate:8e-07\n",
      "()\n",
      "epoch 348, training loss:0.25241780765057936\n",
      "epoch 348, testing loss:0.28707853105692516\n",
      "epoch 348, learning_rate:8e-07\n",
      "()\n",
      "epoch 349, training loss:0.25241780645424255\n",
      "epoch 349, testing loss:0.2870785303658846\n",
      "epoch 349, learning_rate:8e-07\n",
      "()\n",
      "epoch 350, training loss:0.25241780525790586\n",
      "epoch 350, testing loss:0.287078529674844\n",
      "epoch 350, learning_rate:8e-07\n",
      "()\n",
      "epoch 351, training loss:0.2524178051382721\n",
      "epoch 351, testing loss:0.28707852960573993\n",
      "epoch 351, learning_rate:8e-08\n",
      "()\n",
      "epoch 352, training loss:0.2524178050186384\n",
      "epoch 352, testing loss:0.2870785295366359\n",
      "epoch 352, learning_rate:8e-08\n",
      "()\n",
      "epoch 353, training loss:0.2524178048990048\n",
      "epoch 353, testing loss:0.28707852946753176\n",
      "epoch 353, learning_rate:8e-08\n",
      "()\n",
      "epoch 354, training loss:0.2524178047793711\n",
      "epoch 354, testing loss:0.28707852939842776\n",
      "epoch 354, learning_rate:8e-08\n",
      "()\n",
      "epoch 355, training loss:0.25241780465973745\n",
      "epoch 355, testing loss:0.2870785293293237\n",
      "epoch 355, learning_rate:8e-08\n",
      "()\n",
      "epoch 356, training loss:0.25241780454010365\n",
      "epoch 356, testing loss:0.28707852926021965\n",
      "epoch 356, learning_rate:8e-08\n",
      "()\n",
      "epoch 357, training loss:0.25241780442047007\n",
      "epoch 357, testing loss:0.2870785291911156\n",
      "epoch 357, learning_rate:8e-08\n",
      "()\n",
      "epoch 358, training loss:0.2524178043008364\n",
      "epoch 358, testing loss:0.28707852912201154\n",
      "epoch 358, learning_rate:8e-08\n",
      "()\n",
      "epoch 359, training loss:0.25241780418120274\n",
      "epoch 359, testing loss:0.28707852905290754\n",
      "epoch 359, learning_rate:8e-08\n",
      "()\n",
      "epoch 360, training loss:0.252417804061569\n",
      "epoch 360, testing loss:0.28707852898380337\n",
      "epoch 360, learning_rate:8e-08\n",
      "()\n",
      "epoch 361, training loss:0.25241780394193536\n",
      "epoch 361, testing loss:0.2870785289146993\n",
      "epoch 361, learning_rate:8e-08\n",
      "()\n",
      "epoch 362, training loss:0.2524178038223018\n",
      "epoch 362, testing loss:0.28707852884559526\n",
      "epoch 362, learning_rate:8e-08\n",
      "()\n",
      "epoch 363, training loss:0.25241780370266803\n",
      "epoch 363, testing loss:0.2870785287764913\n",
      "epoch 363, learning_rate:8e-08\n",
      "()\n",
      "epoch 364, training loss:0.2524178035830343\n",
      "epoch 364, testing loss:0.2870785287073872\n",
      "epoch 364, learning_rate:8e-08\n",
      "()\n",
      "epoch 365, training loss:0.2524178034634007\n",
      "epoch 365, testing loss:0.2870785286382831\n",
      "epoch 365, learning_rate:8e-08\n",
      "()\n",
      "epoch 366, training loss:0.252417803343767\n",
      "epoch 366, testing loss:0.2870785285691791\n",
      "epoch 366, learning_rate:8e-08\n",
      "()\n",
      "epoch 367, training loss:0.25241780322413343\n",
      "epoch 367, testing loss:0.287078528500075\n",
      "epoch 367, learning_rate:8e-08\n",
      "()\n",
      "epoch 368, training loss:0.2524178031044997\n",
      "epoch 368, testing loss:0.28707852843097104\n",
      "epoch 368, learning_rate:8e-08\n",
      "()\n",
      "epoch 369, training loss:0.25241780298486605\n",
      "epoch 369, testing loss:0.2870785283618669\n",
      "epoch 369, learning_rate:8e-08\n",
      "()\n",
      "epoch 370, training loss:0.25241780286523235\n",
      "epoch 370, testing loss:0.2870785282927629\n",
      "epoch 370, learning_rate:8e-08\n",
      "()\n",
      "epoch 371, training loss:0.2524178027455987\n",
      "epoch 371, testing loss:0.2870785282236589\n",
      "epoch 371, learning_rate:8e-08\n",
      "()\n",
      "epoch 372, training loss:0.2524178026259651\n",
      "epoch 372, testing loss:0.2870785281545548\n",
      "epoch 372, learning_rate:8e-08\n",
      "()\n",
      "epoch 373, training loss:0.2524178025063314\n",
      "epoch 373, testing loss:0.2870785280854508\n",
      "epoch 373, learning_rate:8e-08\n",
      "()\n",
      "epoch 374, training loss:0.2524178023866977\n",
      "epoch 374, testing loss:0.28707852801634665\n",
      "epoch 374, learning_rate:8e-08\n",
      "()\n",
      "epoch 375, training loss:0.25241780226706406\n",
      "epoch 375, testing loss:0.28707852794724265\n",
      "epoch 375, learning_rate:8e-08\n",
      "()\n",
      "epoch 376, training loss:0.25241780214743037\n",
      "epoch 376, testing loss:0.2870785278781386\n",
      "epoch 376, learning_rate:8e-08\n",
      "()\n",
      "epoch 377, training loss:0.25241780202779673\n",
      "epoch 377, testing loss:0.2870785278090346\n",
      "epoch 377, learning_rate:8e-08\n",
      "()\n",
      "epoch 378, training loss:0.2524178019081631\n",
      "epoch 378, testing loss:0.28707852773993053\n",
      "epoch 378, learning_rate:8e-08\n",
      "()\n",
      "epoch 379, training loss:0.25241780178852935\n",
      "epoch 379, testing loss:0.2870785276708265\n",
      "epoch 379, learning_rate:8e-08\n",
      "()\n",
      "epoch 380, training loss:0.2524178016688957\n",
      "epoch 380, testing loss:0.2870785276017224\n",
      "epoch 380, learning_rate:8e-08\n",
      "()\n",
      "epoch 381, training loss:0.2524178015492621\n",
      "epoch 381, testing loss:0.2870785275326183\n",
      "epoch 381, learning_rate:8e-08\n",
      "()\n",
      "epoch 382, training loss:0.2524178014296284\n",
      "epoch 382, testing loss:0.2870785274635143\n",
      "epoch 382, learning_rate:8e-08\n",
      "()\n",
      "epoch 383, training loss:0.25241780130999475\n",
      "epoch 383, testing loss:0.2870785273944102\n",
      "epoch 383, learning_rate:8e-08\n",
      "()\n",
      "epoch 384, training loss:0.25241780119036106\n",
      "epoch 384, testing loss:0.28707852732530625\n",
      "epoch 384, learning_rate:8e-08\n",
      "()\n",
      "epoch 385, training loss:0.25241780107072737\n",
      "epoch 385, testing loss:0.2870785272562022\n",
      "epoch 385, learning_rate:8e-08\n",
      "()\n",
      "epoch 386, training loss:0.2524178009510938\n",
      "epoch 386, testing loss:0.28707852718709814\n",
      "epoch 386, learning_rate:8e-08\n",
      "()\n",
      "epoch 387, training loss:0.2524178008314601\n",
      "epoch 387, testing loss:0.2870785271179941\n",
      "epoch 387, learning_rate:8e-08\n",
      "()\n",
      "epoch 388, training loss:0.2524178007118264\n",
      "epoch 388, testing loss:0.28707852704889003\n",
      "epoch 388, learning_rate:8e-08\n",
      "()\n",
      "epoch 389, training loss:0.25241780059219276\n",
      "epoch 389, testing loss:0.287078526979786\n",
      "epoch 389, learning_rate:8e-08\n",
      "()\n",
      "epoch 390, training loss:0.25241780047255913\n",
      "epoch 390, testing loss:0.287078526910682\n",
      "epoch 390, learning_rate:8e-08\n",
      "()\n",
      "epoch 391, training loss:0.25241780035292544\n",
      "epoch 391, testing loss:0.28707852684157786\n",
      "epoch 391, learning_rate:8e-08\n",
      "()\n",
      "epoch 392, training loss:0.2524178002332918\n",
      "epoch 392, testing loss:0.2870785267724739\n",
      "epoch 392, learning_rate:8e-08\n",
      "()\n",
      "epoch 393, training loss:0.25241780011365805\n",
      "epoch 393, testing loss:0.2870785267033698\n",
      "epoch 393, learning_rate:8e-08\n",
      "()\n",
      "epoch 394, training loss:0.25241779999402447\n",
      "epoch 394, testing loss:0.28707852663426575\n",
      "epoch 394, learning_rate:8e-08\n",
      "()\n",
      "epoch 395, training loss:0.2524177998743908\n",
      "epoch 395, testing loss:0.2870785265651617\n",
      "epoch 395, learning_rate:8e-08\n",
      "()\n",
      "epoch 396, training loss:0.25241779975475714\n",
      "epoch 396, testing loss:0.2870785264960577\n",
      "epoch 396, learning_rate:8e-08\n",
      "()\n",
      "epoch 397, training loss:0.25241779963512345\n",
      "epoch 397, testing loss:0.2870785264269537\n",
      "epoch 397, learning_rate:8e-08\n",
      "()\n",
      "epoch 398, training loss:0.2524177995154898\n",
      "epoch 398, testing loss:0.2870785263578496\n",
      "epoch 398, learning_rate:8e-08\n",
      "()\n",
      "epoch 399, training loss:0.2524177993958562\n",
      "epoch 399, testing loss:0.28707852628874564\n",
      "epoch 399, learning_rate:8e-08\n",
      "()\n",
      "epoch 400, training loss:0.25241779927622254\n",
      "epoch 400, testing loss:0.2870785262196416\n",
      "epoch 400, learning_rate:8e-08\n",
      "()\n",
      "epoch 401, training loss:0.2524177992642591\n",
      "epoch 401, testing loss:0.28707852621273117\n",
      "epoch 401, learning_rate:8e-09\n",
      "()\n",
      "epoch 402, training loss:0.2524177992522958\n",
      "epoch 402, testing loss:0.28707852620582075\n",
      "epoch 402, learning_rate:8e-09\n",
      "()\n",
      "epoch 403, training loss:0.2524177992403324\n",
      "epoch 403, testing loss:0.2870785261989103\n",
      "epoch 403, learning_rate:8e-09\n",
      "()\n",
      "epoch 404, training loss:0.25241779922836904\n",
      "epoch 404, testing loss:0.28707852619199986\n",
      "epoch 404, learning_rate:8e-09\n",
      "()\n",
      "epoch 405, training loss:0.2524177992164057\n",
      "epoch 405, testing loss:0.2870785261850895\n",
      "epoch 405, learning_rate:8e-09\n",
      "()\n",
      "epoch 406, training loss:0.2524177992044423\n",
      "epoch 406, testing loss:0.2870785261781791\n",
      "epoch 406, learning_rate:8e-09\n",
      "()\n",
      "epoch 407, training loss:0.252417799192479\n",
      "epoch 407, testing loss:0.2870785261712687\n",
      "epoch 407, learning_rate:8e-09\n",
      "()\n",
      "epoch 408, training loss:0.2524177991805156\n",
      "epoch 408, testing loss:0.2870785261643583\n",
      "epoch 408, learning_rate:8e-09\n",
      "()\n",
      "epoch 409, training loss:0.25241779916855217\n",
      "epoch 409, testing loss:0.2870785261574479\n",
      "epoch 409, learning_rate:8e-09\n",
      "()\n",
      "epoch 410, training loss:0.2524177991565889\n",
      "epoch 410, testing loss:0.28707852615053747\n",
      "epoch 410, learning_rate:8e-09\n",
      "()\n",
      "epoch 411, training loss:0.2524177991446255\n",
      "epoch 411, testing loss:0.2870785261436271\n",
      "epoch 411, learning_rate:8e-09\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 412, training loss:0.25241779913266216\n",
      "epoch 412, testing loss:0.28707852613671664\n",
      "epoch 412, learning_rate:8e-09\n",
      "()\n",
      "epoch 413, training loss:0.2524177991206987\n",
      "epoch 413, testing loss:0.2870785261298063\n",
      "epoch 413, learning_rate:8e-09\n",
      "()\n",
      "epoch 414, training loss:0.2524177991087354\n",
      "epoch 414, testing loss:0.2870785261228958\n",
      "epoch 414, learning_rate:8e-09\n",
      "()\n",
      "epoch 415, training loss:0.252417799096772\n",
      "epoch 415, testing loss:0.28707852611598544\n",
      "epoch 415, learning_rate:8e-09\n",
      "()\n",
      "epoch 416, training loss:0.2524177990848087\n",
      "epoch 416, testing loss:0.2870785261090751\n",
      "epoch 416, learning_rate:8e-09\n",
      "()\n",
      "epoch 417, training loss:0.25241779907284523\n",
      "epoch 417, testing loss:0.28707852610216467\n",
      "epoch 417, learning_rate:8e-09\n",
      "()\n",
      "epoch 418, training loss:0.252417799060882\n",
      "epoch 418, testing loss:0.2870785260952542\n",
      "epoch 418, learning_rate:8e-09\n",
      "()\n",
      "epoch 419, training loss:0.2524177990489185\n",
      "epoch 419, testing loss:0.2870785260883439\n",
      "epoch 419, learning_rate:8e-09\n",
      "()\n",
      "epoch 420, training loss:0.2524177990369551\n",
      "epoch 420, testing loss:0.2870785260814334\n",
      "epoch 420, learning_rate:8e-09\n",
      "()\n",
      "epoch 421, training loss:0.25241779902499184\n",
      "epoch 421, testing loss:0.28707852607452305\n",
      "epoch 421, learning_rate:8e-09\n",
      "()\n",
      "epoch 422, training loss:0.25241779901302847\n",
      "epoch 422, testing loss:0.28707852606761264\n",
      "epoch 422, learning_rate:8e-09\n",
      "()\n",
      "epoch 423, training loss:0.2524177990010651\n",
      "epoch 423, testing loss:0.28707852606070217\n",
      "epoch 423, learning_rate:8e-09\n",
      "()\n",
      "epoch 424, training loss:0.25241779898910177\n",
      "epoch 424, testing loss:0.28707852605379186\n",
      "epoch 424, learning_rate:8e-09\n",
      "()\n",
      "epoch 425, training loss:0.2524177989771384\n",
      "epoch 425, testing loss:0.2870785260468814\n",
      "epoch 425, learning_rate:8e-09\n",
      "()\n",
      "epoch 426, training loss:0.25241779896517497\n",
      "epoch 426, testing loss:0.287078526039971\n",
      "epoch 426, learning_rate:8e-09\n",
      "()\n",
      "epoch 427, training loss:0.25241779895321165\n",
      "epoch 427, testing loss:0.2870785260330606\n",
      "epoch 427, learning_rate:8e-09\n",
      "()\n",
      "epoch 428, training loss:0.2524177989412483\n",
      "epoch 428, testing loss:0.28707852602615025\n",
      "epoch 428, learning_rate:8e-09\n",
      "()\n",
      "epoch 429, training loss:0.2524177989292849\n",
      "epoch 429, testing loss:0.2870785260192398\n",
      "epoch 429, learning_rate:8e-09\n",
      "()\n",
      "epoch 430, training loss:0.2524177989173215\n",
      "epoch 430, testing loss:0.28707852601232936\n",
      "epoch 430, learning_rate:8e-09\n",
      "()\n",
      "epoch 431, training loss:0.25241779890535815\n",
      "epoch 431, testing loss:0.287078526005419\n",
      "epoch 431, learning_rate:8e-09\n",
      "()\n",
      "epoch 432, training loss:0.2524177988933947\n",
      "epoch 432, testing loss:0.2870785259985086\n",
      "epoch 432, learning_rate:8e-09\n",
      "()\n",
      "epoch 433, training loss:0.25241779888143145\n",
      "epoch 433, testing loss:0.2870785259915981\n",
      "epoch 433, learning_rate:8e-09\n",
      "()\n",
      "epoch 434, training loss:0.2524177988694681\n",
      "epoch 434, testing loss:0.28707852598468775\n",
      "epoch 434, learning_rate:8e-09\n",
      "()\n",
      "epoch 435, training loss:0.2524177988575047\n",
      "epoch 435, testing loss:0.28707852597777744\n",
      "epoch 435, learning_rate:8e-09\n",
      "()\n",
      "epoch 436, training loss:0.25241779884554133\n",
      "epoch 436, testing loss:0.28707852597086697\n",
      "epoch 436, learning_rate:8e-09\n",
      "()\n",
      "epoch 437, training loss:0.252417798833578\n",
      "epoch 437, testing loss:0.28707852596395655\n",
      "epoch 437, learning_rate:8e-09\n",
      "()\n",
      "epoch 438, training loss:0.25241779882161464\n",
      "epoch 438, testing loss:0.2870785259570462\n",
      "epoch 438, learning_rate:8e-09\n",
      "()\n",
      "epoch 439, training loss:0.25241779880965126\n",
      "epoch 439, testing loss:0.2870785259501358\n",
      "epoch 439, learning_rate:8e-09\n",
      "()\n",
      "epoch 440, training loss:0.2524177987976879\n",
      "epoch 440, testing loss:0.28707852594322536\n",
      "epoch 440, learning_rate:8e-09\n",
      "()\n",
      "epoch 441, training loss:0.2524177987857245\n",
      "epoch 441, testing loss:0.28707852593631494\n",
      "epoch 441, learning_rate:8e-09\n",
      "()\n",
      "epoch 442, training loss:0.2524177987737612\n",
      "epoch 442, testing loss:0.2870785259294045\n",
      "epoch 442, learning_rate:8e-09\n",
      "()\n",
      "epoch 443, training loss:0.25241779876179776\n",
      "epoch 443, testing loss:0.28707852592249417\n",
      "epoch 443, learning_rate:8e-09\n",
      "()\n",
      "epoch 444, training loss:0.2524177987498345\n",
      "epoch 444, testing loss:0.28707852591558375\n",
      "epoch 444, learning_rate:8e-09\n",
      "()\n",
      "epoch 445, training loss:0.25241779873787096\n",
      "epoch 445, testing loss:0.28707852590867333\n",
      "epoch 445, learning_rate:8e-09\n",
      "()\n",
      "epoch 446, training loss:0.25241779872590764\n",
      "epoch 446, testing loss:0.2870785259017629\n",
      "epoch 446, learning_rate:8e-09\n",
      "()\n",
      "epoch 447, training loss:0.2524177987139444\n",
      "epoch 447, testing loss:0.28707852589485255\n",
      "epoch 447, learning_rate:8e-09\n",
      "()\n",
      "epoch 448, training loss:0.25241779870198094\n",
      "epoch 448, testing loss:0.28707852588794214\n",
      "epoch 448, learning_rate:8e-09\n",
      "()\n",
      "epoch 449, training loss:0.2524177986900177\n",
      "epoch 449, testing loss:0.28707852588103167\n",
      "epoch 449, learning_rate:8e-09\n",
      "()\n",
      "epoch 450, training loss:0.25241779867805414\n",
      "epoch 450, testing loss:0.28707852587412136\n",
      "epoch 450, learning_rate:8e-09\n",
      "()\n",
      "epoch 451, training loss:0.2524177986768579\n",
      "epoch 451, testing loss:0.28707852587343025\n",
      "epoch 451, learning_rate:8e-10\n",
      "()\n",
      "epoch 452, training loss:0.2524177986756615\n",
      "epoch 452, testing loss:0.28707852587273924\n",
      "epoch 452, learning_rate:8e-10\n",
      "()\n",
      "epoch 453, training loss:0.2524177986744652\n",
      "epoch 453, testing loss:0.2870785258720482\n",
      "epoch 453, learning_rate:8e-10\n",
      "()\n",
      "epoch 454, training loss:0.2524177986732689\n",
      "epoch 454, testing loss:0.2870785258713571\n",
      "epoch 454, learning_rate:8e-10\n",
      "()\n",
      "epoch 455, training loss:0.25241779867207254\n",
      "epoch 455, testing loss:0.2870785258706662\n",
      "epoch 455, learning_rate:8e-10\n",
      "()\n",
      "epoch 456, training loss:0.2524177986708762\n",
      "epoch 456, testing loss:0.28707852586997507\n",
      "epoch 456, learning_rate:8e-10\n",
      "()\n",
      "epoch 457, training loss:0.25241779866967984\n",
      "epoch 457, testing loss:0.28707852586928395\n",
      "epoch 457, learning_rate:8e-10\n",
      "()\n",
      "epoch 458, training loss:0.2524177986684836\n",
      "epoch 458, testing loss:0.287078525868593\n",
      "epoch 458, learning_rate:8e-10\n",
      "()\n",
      "epoch 459, training loss:0.2524177986672872\n",
      "epoch 459, testing loss:0.287078525867902\n",
      "epoch 459, learning_rate:8e-10\n",
      "()\n",
      "epoch 460, training loss:0.2524177986660909\n",
      "epoch 460, testing loss:0.28707852586721083\n",
      "epoch 460, learning_rate:8e-10\n",
      "()\n",
      "epoch 461, training loss:0.25241779866489455\n",
      "epoch 461, testing loss:0.28707852586651983\n",
      "epoch 461, learning_rate:8e-10\n",
      "()\n",
      "epoch 462, training loss:0.25241779866369823\n",
      "epoch 462, testing loss:0.2870785258658288\n",
      "epoch 462, learning_rate:8e-10\n",
      "()\n",
      "epoch 463, training loss:0.25241779866250186\n",
      "epoch 463, testing loss:0.2870785258651378\n",
      "epoch 463, learning_rate:8e-10\n",
      "()\n",
      "epoch 464, training loss:0.25241779866130554\n",
      "epoch 464, testing loss:0.28707852586444677\n",
      "epoch 464, learning_rate:8e-10\n",
      "()\n",
      "epoch 465, training loss:0.25241779866010916\n",
      "epoch 465, testing loss:0.2870785258637557\n",
      "epoch 465, learning_rate:8e-10\n",
      "()\n",
      "epoch 466, training loss:0.2524177986589128\n",
      "epoch 466, testing loss:0.2870785258630647\n",
      "epoch 466, learning_rate:8e-10\n",
      "()\n",
      "epoch 467, training loss:0.2524177986577165\n",
      "epoch 467, testing loss:0.28707852586237365\n",
      "epoch 467, learning_rate:8e-10\n",
      "()\n",
      "epoch 468, training loss:0.25241779865652014\n",
      "epoch 468, testing loss:0.28707852586168253\n",
      "epoch 468, learning_rate:8e-10\n",
      "()\n",
      "epoch 469, training loss:0.2524177986553239\n",
      "epoch 469, testing loss:0.28707852586099153\n",
      "epoch 469, learning_rate:8e-10\n",
      "()\n",
      "epoch 470, training loss:0.25241779865412745\n",
      "epoch 470, testing loss:0.2870785258603005\n",
      "epoch 470, learning_rate:8e-10\n",
      "()\n",
      "epoch 471, training loss:0.2524177986529311\n",
      "epoch 471, testing loss:0.28707852585960947\n",
      "epoch 471, learning_rate:8e-10\n",
      "()\n",
      "epoch 472, training loss:0.2524177986517348\n",
      "epoch 472, testing loss:0.28707852585891847\n",
      "epoch 472, learning_rate:8e-10\n",
      "()\n",
      "epoch 473, training loss:0.2524177986505385\n",
      "epoch 473, testing loss:0.28707852585822746\n",
      "epoch 473, learning_rate:8e-10\n",
      "()\n",
      "epoch 474, training loss:0.2524177986493421\n",
      "epoch 474, testing loss:0.2870785258575364\n",
      "epoch 474, learning_rate:8e-10\n",
      "()\n",
      "epoch 475, training loss:0.25241779864814573\n",
      "epoch 475, testing loss:0.2870785258568453\n",
      "epoch 475, learning_rate:8e-10\n",
      "()\n",
      "epoch 476, training loss:0.2524177986469494\n",
      "epoch 476, testing loss:0.2870785258561543\n",
      "epoch 476, learning_rate:8e-10\n",
      "()\n",
      "epoch 477, training loss:0.2524177986457531\n",
      "epoch 477, testing loss:0.2870785258554633\n",
      "epoch 477, learning_rate:8e-10\n",
      "()\n",
      "epoch 478, training loss:0.25241779864455677\n",
      "epoch 478, testing loss:0.2870785258547722\n",
      "epoch 478, learning_rate:8e-10\n",
      "()\n",
      "epoch 479, training loss:0.2524177986433604\n",
      "epoch 479, testing loss:0.28707852585408117\n",
      "epoch 479, learning_rate:8e-10\n",
      "()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 480, training loss:0.2524177986421641\n",
      "epoch 480, testing loss:0.2870785258533901\n",
      "epoch 480, learning_rate:8e-10\n",
      "()\n",
      "epoch 481, training loss:0.25241779864096786\n",
      "epoch 481, testing loss:0.287078525852699\n",
      "epoch 481, learning_rate:8e-10\n",
      "()\n",
      "epoch 482, training loss:0.2524177986397715\n",
      "epoch 482, testing loss:0.28707852585200805\n",
      "epoch 482, learning_rate:8e-10\n",
      "()\n",
      "epoch 483, training loss:0.2524177986385752\n",
      "epoch 483, testing loss:0.28707852585131705\n",
      "epoch 483, learning_rate:8e-10\n",
      "()\n",
      "epoch 484, training loss:0.25241779863737873\n",
      "epoch 484, testing loss:0.28707852585062593\n",
      "epoch 484, learning_rate:8e-10\n",
      "()\n",
      "epoch 485, training loss:0.2524177986361824\n",
      "epoch 485, testing loss:0.28707852584993493\n",
      "epoch 485, learning_rate:8e-10\n",
      "()\n",
      "epoch 486, training loss:0.25241779863498615\n",
      "epoch 486, testing loss:0.28707852584924387\n",
      "epoch 486, learning_rate:8e-10\n",
      "()\n",
      "epoch 487, training loss:0.25241779863378977\n",
      "epoch 487, testing loss:0.2870785258485528\n",
      "epoch 487, learning_rate:8e-10\n",
      "()\n",
      "epoch 488, training loss:0.25241779863259345\n",
      "epoch 488, testing loss:0.28707852584786175\n",
      "epoch 488, learning_rate:8e-10\n",
      "()\n",
      "epoch 489, training loss:0.2524177986313972\n",
      "epoch 489, testing loss:0.28707852584717075\n",
      "epoch 489, learning_rate:8e-10\n",
      "()\n",
      "epoch 490, training loss:0.2524177986302007\n",
      "epoch 490, testing loss:0.2870785258464797\n",
      "epoch 490, learning_rate:8e-10\n",
      "()\n",
      "epoch 491, training loss:0.2524177986290043\n",
      "epoch 491, testing loss:0.2870785258457887\n",
      "epoch 491, learning_rate:8e-10\n",
      "()\n",
      "epoch 492, training loss:0.252417798627808\n",
      "epoch 492, testing loss:0.28707852584509763\n",
      "epoch 492, learning_rate:8e-10\n",
      "()\n",
      "epoch 493, training loss:0.25241779862661173\n",
      "epoch 493, testing loss:0.2870785258444066\n",
      "epoch 493, learning_rate:8e-10\n",
      "()\n",
      "epoch 494, training loss:0.25241779862541547\n",
      "epoch 494, testing loss:0.28707852584371557\n",
      "epoch 494, learning_rate:8e-10\n",
      "()\n",
      "epoch 495, training loss:0.25241779862421904\n",
      "epoch 495, testing loss:0.2870785258430245\n",
      "epoch 495, learning_rate:8e-10\n",
      "()\n",
      "epoch 496, training loss:0.2524177986230227\n",
      "epoch 496, testing loss:0.2870785258423335\n",
      "epoch 496, learning_rate:8e-10\n",
      "()\n",
      "epoch 497, training loss:0.2524177986218264\n",
      "epoch 497, testing loss:0.2870785258416425\n",
      "epoch 497, learning_rate:8e-10\n",
      "()\n",
      "epoch 498, training loss:0.25241779862063\n",
      "epoch 498, testing loss:0.2870785258409514\n",
      "epoch 498, learning_rate:8e-10\n",
      "()\n",
      "epoch 499, training loss:0.2524177986194337\n",
      "epoch 499, testing loss:0.2870785258402604\n",
      "epoch 499, learning_rate:8e-10\n",
      "()\n",
      "epoch 500, training loss:0.25241779861823743\n",
      "epoch 500, testing loss:0.28707852583956933\n",
      "epoch 500, learning_rate:8e-10\n",
      "()\n",
      "training time: 535.240813971 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "h = 50\n",
    "K = 10\n",
    "parameters = initialize(h, K)\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 500, 0.8, True)\n",
    "\n",
    "end_time = time()\n",
    "print('training time: %s s'%(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集:\n",
      "0.9146825396825397\n",
      "训练集:\n",
      "0.9263945578231293\n"
     ]
    }
   ],
   "source": [
    "# 精度\n",
    "from sklearn.metrics import accuracy_score\n",
    "prediction = predict(testX, parameters)\n",
    "print(\"测试集:\")\n",
    "print(accuracy_score(prediction, testY))\n",
    "print(\"训练集:\")\n",
    "print(accuracy_score(predict(trainX, parameters), trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制曲线\n",
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用sklearn中的MLRClassifier模型实现学习率变化\n",
    "\n",
    "根据查阅官方文档定义得到一下参数作用：\n",
    "1. activation：用于决定激活函数默认为relu，本处修改为logistic\n",
    "2. solver：用于决定优化算法，默认为adam，本处修改为sgd\n",
    "3. learning_rate：'constant'，学习率固定，不衰减\n",
    "4. hidden_layer_sizes:隐藏层层数和每层隐藏层神经元个数，本处设置为3层每层神经元个数依次为512,128,32\n",
    "5. learning_rate_init，这个参数需要我们进行调整，这是学习率，本处为0.1\n",
    "6. max_iter: 设定最大迭代轮数，如果超过这个轮数还没有收敛，就停止训练，并抛出一个warning\n",
    "7. learning_rate：表示学习率的变化情况（当且仅当损失函数为sgd时可用）可选值为：\n",
    "    1. constant:训练过程中学习率不变（默认）\n",
    "    2. invscaling:随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_,effective_learning_rate=learning_rate_init/pow(t,power_t)\n",
    "    3. adaptive:只要训练误差持续下降就保持不变否则发生变化。当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. \n",
    "    \n",
    "本模型为多层（5层）感知机，其由输入层、3层隐藏层和输出层组成，使用logistic激活函数与随机梯度下降优化算法作为损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12600,)\n",
      "(12600,)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(activation = \"logistic\",solver = 'sgd', learning_rate = 'adaptive', hidden_layer_sizes=(512,128,32), learning_rate_init=1.5, max_iter=500)\n",
    "model.fit(trainX, trainY)\n",
    "prediction = model.predict(testX)\n",
    "print(prediction.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9651587301587301"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998979591836735"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(trainX), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'loss')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
